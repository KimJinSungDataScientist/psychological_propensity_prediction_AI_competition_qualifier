{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train = pd.read_csv('./data/train.csv').drop('index',axis=1)\n",
    "# train.drop('introelapse',axis=1,inplace=True)\n",
    "# family size 이상치\n",
    "train = train.drop(1019)\n",
    "# age 이상치\n",
    "train = train.drop(train[train['age']>100].index.to_list())\n",
    "train = train.drop(train[train['introelapse']>3000].index.to_list())\n",
    "train = train.drop(train[train['testelapse']>10000].index.to_list())\n",
    "train = train.drop(train[train['surveyelapse']>10000].index.to_list()) # 나에 대한 추가 질문\n",
    "\n",
    "test = pd.read_csv('./data/test.csv').drop('index',axis=1)\n",
    "# test.drop('introelapse',axis=1,inplace=True)\n",
    "test.loc[test['familysize']>100,'familysize']= train['familysize'].mean()\n",
    "test.loc[test['age']>100,'age']= train['age'].mean()\n",
    "test.loc[test['introelapse']>3000,'introelapse']= train['introelapse'].mean()\n",
    "test.loc[test['testelapse']>10000,'testelapse']= train['testelapse'].mean()\n",
    "test.loc[test['surveyelapse']>10000,'surveyelapse']= train['surveyelapse'].mean()\n",
    "\n",
    "test_index = pd.read_csv('./data/test.csv')['index']\n",
    "\n",
    "pd.set_option('display.max_row', 100)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "value = train['country'].value_counts().values\n",
    "rank = list(map(lambda x : 1 if x>1000 else (2 if x>100 else 3),value))\n",
    "# rank = list(map(lambda x : 1 if x>2000 else (2 if x>1000 else (3 if x>500 else (4 if x>200 else (5 if x>100 else 6)))),value))\n",
    "\n",
    "temp_dict = {i : 0 for i in train['country'].value_counts().index.to_list()}\n",
    "\n",
    "rank_dict = dict(zip(train['country'].value_counts().index.to_list(), rank))\n",
    "rank_dict['nan'] = 0\n",
    "train['country'] = train['country'].fillna('nan')\n",
    "train['country'] = train['country'].apply(lambda x : rank_dict[x])\n",
    "train['Ex'] = (train['TIPI1']+train['TIPI6'])/2\n",
    "train['Ag'] = (train['TIPI7']+train['TIPI2'])/2\n",
    "train['Con'] = (train['TIPI3']+train['TIPI8'])/2\n",
    "train['Es'] =(train['TIPI9']+train['TIPI4'])/2\n",
    "train['Op'] =(train['TIPI5']+train['TIPI10'])/2\n",
    "\n",
    "train['mach_score'] = train[train.columns[:20]].apply(lambda x : x.mean(),axis=1)\n",
    "train['T'] = train['Q1'] + train['Q2'] - train['Q3'] - train['Q6'] - train['Q7'] - train['Q10'] + train['Q12'] + train['Q15'] - train['Q16']\n",
    "train['V'] = -train['Q4'] + train['Q5'] + train['Q8'] - train['Q11'] + train['Q13'] - train['Q14'] - train['Q17'] + train['Q18'] + train['Q20']\n",
    "train['M'] = -train['Q9'] + train['Q19']\n",
    "train['introelapse'] = np.log1p(train['introelapse'])\n",
    "train['testelapse'] = np.log1p(train['testelapse'])\n",
    "train['surveyelapse'] = np.log1p(train['surveyelapse'])\n",
    "# train.drop('hand',axis=1,inplace=True)\n",
    "# train.drop(['VCL7','VCL8','VCL11'],axis=1,inplace=True)\n",
    "# train.drop([('TIPI'+str(i)) for i in range(1,11)],axis=1,inplace=True)\n",
    "# train.drop([('Q'+str(i)) for i in range(1,20)],axis=1,inplace=True)\n",
    "\n",
    "train['nature_score'] = train[[('Q'+str(i)) for i in range(20,27)]].apply(lambda x : x.mean(),axis=1)\n",
    "train.drop([('Q'+str(i)) for i in range(20,27)],axis=1,inplace=True)\n",
    "train_fill_na = train.fillna(train.mean())\n",
    "\n",
    "\n",
    "value = test['country'].value_counts().values\n",
    "rank = list(map(lambda x : 1 if x>1000 else (2 if x>100 else 3),value))\n",
    "# rank = list(map(lambda x : 1 if x>2000 else (2 if x>1000 else (3 if x>500 else (4 if x>200 else (5 if x>100 else 6)))),value))\n",
    "\n",
    "temp_dict = {i : 0 for i in test['country'].value_counts().index.to_list()}\n",
    "\n",
    "rank_dict = dict(zip(test['country'].value_counts().index.to_list(), rank))\n",
    "rank_dict['nan'] = 0\n",
    "test['country'] = test['country'].fillna('nan')\n",
    "test['country'] = test['country'].apply(lambda x : rank_dict[x])\n",
    "\n",
    "test['Ex'] = (test['TIPI1']+test['TIPI6'])/2\n",
    "test['Ag'] = (test['TIPI7']+test['TIPI2'])/2\n",
    "test['Con'] = (test['TIPI3']+test['TIPI8'])/2\n",
    "test['Es'] =(test['TIPI9']+test['TIPI4'])/2\n",
    "test['Op'] =(test['TIPI5']+test['TIPI10'])/2\n",
    "\n",
    "test['mach_score'] = test[test.columns[:20]].apply(lambda x : x.mean(),axis=1)\n",
    "test['T'] = test['Q1'] + test['Q2'] - test['Q3'] - test['Q6'] - test['Q7'] - test['Q10'] + test['Q12'] + test['Q15'] - test['Q16']\n",
    "test['V'] = -test['Q4'] + test['Q5'] + test['Q8'] - test['Q11'] + test['Q13'] - test['Q14'] - test['Q17'] + test['Q18'] + test['Q20']\n",
    "test['M'] = -test['Q9'] + test['Q19']\n",
    "\n",
    "test['introelapse'] = np.log1p(test['introelapse'])\n",
    "test['testelapse'] = np.log1p(test['testelapse'])\n",
    "test['surveyelapse'] = np.log1p(test['surveyelapse'])\n",
    "# test.drop('hand',axis=1,inplace=True)\n",
    "# test.drop(['VCL7','VCL8','VCL11'],axis=1,inplace=True)\n",
    "# test.drop([('TIPI'+str(i)) for i in range(1,11)],axis=1,inplace=True)\n",
    "# test.drop([('Q'+str(i)) for i in range(1,20)],axis=1,inplace=True)\n",
    "\n",
    "test['nature_score'] = test[[('Q'+str(i)) for i in range(20,27)]].apply(lambda x : x.mean(),axis=1)\n",
    "test.drop([('Q'+str(i)) for i in range(20,27)],axis=1,inplace=True)\n",
    "test_fill_na = test.fillna(test.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from string import ascii_lowercase\n",
    "from itertools import combinations\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import  GradientBoostingClassifier\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0    15360\n",
       "4.0     9364\n",
       "5.0     6188\n",
       "2.0     2875\n",
       "1.0     1502\n",
       "Name: TIPI3, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('./data/test.csv').drop('index',axis=1)\n",
    "test['TIPI3'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train = pd.read_csv('./data/train.csv').drop('index',axis=1)\n",
    "# train.drop('introelapse',axis=1,inplace=True)\n",
    "# family size 이상치\n",
    "train = train.drop(1019)\n",
    "# age 이상치\n",
    "train = train.drop(train[train['age']>100].index.to_list())\n",
    "train = train.drop(train[train['introelapse']>3000].index.to_list())\n",
    "train = train.drop(train[train['testelapse']>10000].index.to_list())\n",
    "train = train.drop(train[train['surveyelapse']>10000].index.to_list()) # 나에 대한 추가 질문\n",
    "\n",
    "test = pd.read_csv('./data/test.csv').drop('index',axis=1)\n",
    "# test.drop('introelapse',axis=1,inplace=True)\n",
    "test.loc[test['familysize']>100,'familysize']= train['familysize'].mean()\n",
    "test.loc[test['age']>100,'age']= train['age'].mean()\n",
    "test.loc[test['introelapse']>3000,'introelapse']= train['introelapse'].mean()\n",
    "test.loc[test['testelapse']>10000,'testelapse']= train['testelapse'].mean()\n",
    "test.loc[test['surveyelapse']>10000,'surveyelapse']= train['surveyelapse'].mean()\n",
    "\n",
    "test_index = pd.read_csv('./data/test.csv')['index']\n",
    "\n",
    "pd.set_option('display.max_row', 100)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "value = train['country'].value_counts().values\n",
    "rank = list(map(lambda x : 1 if x>1000 else (2 if x>100 else 3),value))\n",
    "# rank = list(map(lambda x : 1 if x>2000 else (2 if x>1000 else (3 if x>500 else (4 if x>200 else (5 if x>100 else 6)))),value))\n",
    "\n",
    "temp_dict = {i : 0 for i in train['country'].value_counts().index.to_list()}\n",
    "\n",
    "rank_dict = dict(zip(train['country'].value_counts().index.to_list(), rank))\n",
    "rank_dict['nan'] = 0\n",
    "train['country'] = train['country'].fillna('nan')\n",
    "train['country'] = train['country'].apply(lambda x : rank_dict[x])\n",
    "train['Ex'] = (train['TIPI1']+train['TIPI6'])/2\n",
    "train['Ag'] = (train['TIPI7']+train['TIPI2'])/2\n",
    "train['Con'] = (train['TIPI3']+train['TIPI8'])/2\n",
    "train['Es'] =(train['TIPI9']+train['TIPI4'])/2\n",
    "train['Op'] =(train['TIPI5']+train['TIPI10'])/2\n",
    "\n",
    "train['mach_score'] = train[train.columns[:20]].apply(lambda x : x.mean(),axis=1)\n",
    "train['T'] = train['Q1'] + train['Q2'] - train['Q3'] - train['Q6'] - train['Q7'] - train['Q10'] + train['Q12'] + train['Q15'] - train['Q16']\n",
    "train['V'] = -train['Q4'] + train['Q5'] + train['Q8'] - train['Q11'] + train['Q13'] - train['Q14'] - train['Q17'] + train['Q18'] + train['Q20']\n",
    "train['M'] = -train['Q9'] + train['Q19']\n",
    "train['introelapse'] = np.log1p(train['introelapse'])\n",
    "train['testelapse'] = np.log1p(train['testelapse'])\n",
    "train['surveyelapse'] = np.log1p(train['surveyelapse'])\n",
    "train.drop('hand',axis=1,inplace=True)\n",
    "train.drop(['VCL7','VCL8','VCL11'],axis=1,inplace=True)\n",
    "train.drop([('TIPI'+str(i)) for i in range(1,11)],axis=1,inplace=True)\n",
    "train.drop([('Q'+str(i)) for i in range(1,20)],axis=1,inplace=True)\n",
    "\n",
    "train['nature_score'] = train[[('Q'+str(i)) for i in range(20,27)]].apply(lambda x : x.mean(),axis=1)\n",
    "train.drop([('Q'+str(i)) for i in range(20,27)],axis=1,inplace=True)\n",
    "train_fill_na = train.fillna(train.mean())\n",
    "\n",
    "\n",
    "value = test['country'].value_counts().values\n",
    "rank = list(map(lambda x : 1 if x>1000 else (2 if x>100 else 3),value))\n",
    "# rank = list(map(lambda x : 1 if x>2000 else (2 if x>1000 else (3 if x>500 else (4 if x>200 else (5 if x>100 else 6)))),value))\n",
    "\n",
    "temp_dict = {i : 0 for i in test['country'].value_counts().index.to_list()}\n",
    "\n",
    "rank_dict = dict(zip(test['country'].value_counts().index.to_list(), rank))\n",
    "rank_dict['nan'] = 0\n",
    "test['country'] = test['country'].fillna('nan')\n",
    "test['country'] = test['country'].apply(lambda x : rank_dict[x])\n",
    "\n",
    "test['Ex'] = (test['TIPI1']+test['TIPI6'])/2\n",
    "test['Ag'] = (test['TIPI7']+test['TIPI2'])/2\n",
    "test['Con'] = (test['TIPI3']+test['TIPI8'])/2\n",
    "test['Es'] =(test['TIPI9']+test['TIPI4'])/2\n",
    "test['Op'] =(test['TIPI5']+test['TIPI10'])/2\n",
    "\n",
    "test['mach_score'] = test[test.columns[:20]].apply(lambda x : x.mean(),axis=1)\n",
    "test['T'] = test['Q1'] + test['Q2'] - test['Q3'] - test['Q6'] - test['Q7'] - test['Q10'] + test['Q12'] + test['Q15'] - test['Q16']\n",
    "test['V'] = -test['Q4'] + test['Q5'] + test['Q8'] - test['Q11'] + test['Q13'] - test['Q14'] - test['Q17'] + test['Q18'] + test['Q20']\n",
    "test['M'] = -test['Q9'] + test['Q19']\n",
    "\n",
    "test['introelapse'] = np.log1p(test['introelapse'])\n",
    "test['testelapse'] = np.log1p(test['testelapse'])\n",
    "test['surveyelapse'] = np.log1p(test['surveyelapse'])\n",
    "test.drop('hand',axis=1,inplace=True)\n",
    "test.drop(['VCL7','VCL8','VCL11'],axis=1,inplace=True)\n",
    "test.drop([('TIPI'+str(i)) for i in range(1,11)],axis=1,inplace=True)\n",
    "test.drop([('Q'+str(i)) for i in range(1,20)],axis=1,inplace=True)\n",
    "\n",
    "test['nature_score'] = test[[('Q'+str(i)) for i in range(20,27)]].apply(lambda x : x.mean(),axis=1)\n",
    "test.drop([('Q'+str(i)) for i in range(20,27)],axis=1,inplace=True)\n",
    "test_fill_na = test.fillna(test.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_a82ad_row9_col1 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_a82ad\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_a82ad_level0_col0\" class=\"col_heading level0 col0\" >Description</th>\n",
       "      <th id=\"T_a82ad_level0_col1\" class=\"col_heading level0 col1\" >Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_a82ad_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_a82ad_row0_col0\" class=\"data row0 col0\" >Session id</td>\n",
       "      <td id=\"T_a82ad_row0_col1\" class=\"data row0 col1\" >1865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a82ad_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_a82ad_row1_col0\" class=\"data row1 col0\" >Target</td>\n",
       "      <td id=\"T_a82ad_row1_col1\" class=\"data row1 col1\" >nerdiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a82ad_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_a82ad_row2_col0\" class=\"data row2 col0\" >Target type</td>\n",
       "      <td id=\"T_a82ad_row2_col1\" class=\"data row2 col1\" >Binary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a82ad_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_a82ad_row3_col0\" class=\"data row3 col0\" >Original data shape</td>\n",
       "      <td id=\"T_a82ad_row3_col1\" class=\"data row3 col1\" >(14961, 74)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a82ad_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_a82ad_row4_col0\" class=\"data row4 col0\" >Transformed data shape</td>\n",
       "      <td id=\"T_a82ad_row4_col1\" class=\"data row4 col1\" >(14961, 74)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a82ad_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_a82ad_row5_col0\" class=\"data row5 col0\" >Transformed train set shape</td>\n",
       "      <td id=\"T_a82ad_row5_col1\" class=\"data row5 col1\" >(10472, 74)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a82ad_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_a82ad_row6_col0\" class=\"data row6 col0\" >Transformed test set shape</td>\n",
       "      <td id=\"T_a82ad_row6_col1\" class=\"data row6 col1\" >(4489, 74)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a82ad_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_a82ad_row7_col0\" class=\"data row7 col0\" >Numeric features</td>\n",
       "      <td id=\"T_a82ad_row7_col1\" class=\"data row7 col1\" >73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a82ad_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_a82ad_row8_col0\" class=\"data row8 col0\" >Rows with missing values</td>\n",
       "      <td id=\"T_a82ad_row8_col1\" class=\"data row8 col1\" >14.7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a82ad_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_a82ad_row9_col0\" class=\"data row9 col0\" >Preprocess</td>\n",
       "      <td id=\"T_a82ad_row9_col1\" class=\"data row9 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a82ad_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_a82ad_row10_col0\" class=\"data row10 col0\" >Imputation type</td>\n",
       "      <td id=\"T_a82ad_row10_col1\" class=\"data row10 col1\" >simple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a82ad_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_a82ad_row11_col0\" class=\"data row11 col0\" >Numeric imputation</td>\n",
       "      <td id=\"T_a82ad_row11_col1\" class=\"data row11 col1\" >mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a82ad_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_a82ad_row12_col0\" class=\"data row12 col0\" >Categorical imputation</td>\n",
       "      <td id=\"T_a82ad_row12_col1\" class=\"data row12 col1\" >constant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a82ad_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_a82ad_row13_col0\" class=\"data row13 col0\" >Low variance threshold</td>\n",
       "      <td id=\"T_a82ad_row13_col1\" class=\"data row13 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a82ad_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_a82ad_row14_col0\" class=\"data row14 col0\" >Fold Generator</td>\n",
       "      <td id=\"T_a82ad_row14_col1\" class=\"data row14 col1\" >StratifiedKFold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a82ad_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_a82ad_row15_col0\" class=\"data row15 col0\" >Fold Number</td>\n",
       "      <td id=\"T_a82ad_row15_col1\" class=\"data row15 col1\" >10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a82ad_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_a82ad_row16_col0\" class=\"data row16 col0\" >CPU Jobs</td>\n",
       "      <td id=\"T_a82ad_row16_col1\" class=\"data row16 col1\" >-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a82ad_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_a82ad_row17_col0\" class=\"data row17 col0\" >Use GPU</td>\n",
       "      <td id=\"T_a82ad_row17_col1\" class=\"data row17 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a82ad_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_a82ad_row18_col0\" class=\"data row18 col0\" >Log Experiment</td>\n",
       "      <td id=\"T_a82ad_row18_col1\" class=\"data row18 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a82ad_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_a82ad_row19_col0\" class=\"data row19 col0\" >Experiment Name</td>\n",
       "      <td id=\"T_a82ad_row19_col1\" class=\"data row19 col1\" >clf-default-name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a82ad_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "      <td id=\"T_a82ad_row20_col0\" class=\"data row20 col0\" >USI</td>\n",
       "      <td id=\"T_a82ad_row20_col1\" class=\"data row20 col1\" >57bb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f5e65be0b50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pycaret.classification import *\n",
    "\n",
    "clf = setup(data = train, target = 'nerdiness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_e3ce1 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_e3ce1_row0_col0, #T_e3ce1_row0_col1, #T_e3ce1_row0_col3, #T_e3ce1_row0_col4, #T_e3ce1_row0_col5, #T_e3ce1_row0_col6, #T_e3ce1_row0_col7, #T_e3ce1_row1_col0, #T_e3ce1_row1_col2, #T_e3ce1_row1_col3, #T_e3ce1_row2_col0, #T_e3ce1_row2_col1, #T_e3ce1_row2_col2, #T_e3ce1_row2_col3, #T_e3ce1_row2_col4, #T_e3ce1_row2_col5, #T_e3ce1_row2_col6, #T_e3ce1_row2_col7, #T_e3ce1_row3_col0, #T_e3ce1_row3_col1, #T_e3ce1_row3_col2, #T_e3ce1_row3_col3, #T_e3ce1_row3_col4, #T_e3ce1_row3_col5, #T_e3ce1_row3_col6, #T_e3ce1_row3_col7, #T_e3ce1_row4_col0, #T_e3ce1_row4_col1, #T_e3ce1_row4_col2, #T_e3ce1_row4_col3, #T_e3ce1_row4_col4, #T_e3ce1_row4_col5, #T_e3ce1_row4_col6, #T_e3ce1_row4_col7, #T_e3ce1_row5_col0, #T_e3ce1_row5_col1, #T_e3ce1_row5_col2, #T_e3ce1_row5_col3, #T_e3ce1_row5_col4, #T_e3ce1_row5_col5, #T_e3ce1_row5_col6, #T_e3ce1_row5_col7, #T_e3ce1_row6_col0, #T_e3ce1_row6_col1, #T_e3ce1_row6_col2, #T_e3ce1_row6_col3, #T_e3ce1_row6_col4, #T_e3ce1_row6_col5, #T_e3ce1_row6_col6, #T_e3ce1_row6_col7, #T_e3ce1_row7_col0, #T_e3ce1_row7_col1, #T_e3ce1_row7_col2, #T_e3ce1_row7_col3, #T_e3ce1_row7_col4, #T_e3ce1_row7_col5, #T_e3ce1_row7_col6, #T_e3ce1_row7_col7, #T_e3ce1_row8_col0, #T_e3ce1_row8_col1, #T_e3ce1_row8_col2, #T_e3ce1_row8_col3, #T_e3ce1_row8_col4, #T_e3ce1_row8_col5, #T_e3ce1_row8_col6, #T_e3ce1_row8_col7, #T_e3ce1_row9_col0, #T_e3ce1_row9_col1, #T_e3ce1_row9_col2, #T_e3ce1_row9_col3, #T_e3ce1_row9_col4, #T_e3ce1_row9_col5, #T_e3ce1_row9_col6, #T_e3ce1_row9_col7, #T_e3ce1_row10_col0, #T_e3ce1_row10_col1, #T_e3ce1_row10_col2, #T_e3ce1_row10_col3, #T_e3ce1_row10_col4, #T_e3ce1_row10_col5, #T_e3ce1_row10_col6, #T_e3ce1_row10_col7, #T_e3ce1_row11_col0, #T_e3ce1_row11_col1, #T_e3ce1_row11_col2, #T_e3ce1_row11_col3, #T_e3ce1_row11_col4, #T_e3ce1_row11_col5, #T_e3ce1_row11_col6, #T_e3ce1_row11_col7, #T_e3ce1_row12_col0, #T_e3ce1_row12_col1, #T_e3ce1_row12_col2, #T_e3ce1_row12_col4, #T_e3ce1_row12_col5, #T_e3ce1_row12_col6, #T_e3ce1_row12_col7, #T_e3ce1_row13_col0, #T_e3ce1_row13_col1, #T_e3ce1_row13_col2, #T_e3ce1_row13_col3, #T_e3ce1_row13_col4, #T_e3ce1_row13_col5, #T_e3ce1_row13_col6, #T_e3ce1_row13_col7, #T_e3ce1_row14_col0, #T_e3ce1_row14_col1, #T_e3ce1_row14_col2, #T_e3ce1_row14_col3, #T_e3ce1_row14_col4, #T_e3ce1_row14_col5, #T_e3ce1_row14_col6, #T_e3ce1_row14_col7 {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_e3ce1_row0_col2, #T_e3ce1_row1_col1, #T_e3ce1_row1_col4, #T_e3ce1_row1_col5, #T_e3ce1_row1_col6, #T_e3ce1_row1_col7, #T_e3ce1_row12_col3 {\n",
       "  text-align: left;\n",
       "  background-color: yellow;\n",
       "}\n",
       "#T_e3ce1_row0_col8, #T_e3ce1_row1_col8, #T_e3ce1_row2_col8, #T_e3ce1_row3_col8, #T_e3ce1_row4_col8, #T_e3ce1_row5_col8, #T_e3ce1_row6_col8, #T_e3ce1_row7_col8, #T_e3ce1_row8_col8, #T_e3ce1_row9_col8, #T_e3ce1_row10_col8, #T_e3ce1_row11_col8, #T_e3ce1_row13_col8, #T_e3ce1_row14_col8 {\n",
       "  text-align: left;\n",
       "  background-color: lightgrey;\n",
       "}\n",
       "#T_e3ce1_row12_col8 {\n",
       "  text-align: left;\n",
       "  background-color: yellow;\n",
       "  background-color: lightgrey;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_e3ce1\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_e3ce1_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_e3ce1_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_e3ce1_level0_col2\" class=\"col_heading level0 col2\" >AUC</th>\n",
       "      <th id=\"T_e3ce1_level0_col3\" class=\"col_heading level0 col3\" >Recall</th>\n",
       "      <th id=\"T_e3ce1_level0_col4\" class=\"col_heading level0 col4\" >Prec.</th>\n",
       "      <th id=\"T_e3ce1_level0_col5\" class=\"col_heading level0 col5\" >F1</th>\n",
       "      <th id=\"T_e3ce1_level0_col6\" class=\"col_heading level0 col6\" >Kappa</th>\n",
       "      <th id=\"T_e3ce1_level0_col7\" class=\"col_heading level0 col7\" >MCC</th>\n",
       "      <th id=\"T_e3ce1_level0_col8\" class=\"col_heading level0 col8\" >TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_e3ce1_level0_row0\" class=\"row_heading level0 row0\" >et</th>\n",
       "      <td id=\"T_e3ce1_row0_col0\" class=\"data row0 col0\" >Extra Trees Classifier</td>\n",
       "      <td id=\"T_e3ce1_row0_col1\" class=\"data row0 col1\" >0.7674</td>\n",
       "      <td id=\"T_e3ce1_row0_col2\" class=\"data row0 col2\" >0.8580</td>\n",
       "      <td id=\"T_e3ce1_row0_col3\" class=\"data row0 col3\" >0.8261</td>\n",
       "      <td id=\"T_e3ce1_row0_col4\" class=\"data row0 col4\" >0.7703</td>\n",
       "      <td id=\"T_e3ce1_row0_col5\" class=\"data row0 col5\" >0.7972</td>\n",
       "      <td id=\"T_e3ce1_row0_col6\" class=\"data row0 col6\" >0.5252</td>\n",
       "      <td id=\"T_e3ce1_row0_col7\" class=\"data row0 col7\" >0.5271</td>\n",
       "      <td id=\"T_e3ce1_row0_col8\" class=\"data row0 col8\" >0.1390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e3ce1_level0_row1\" class=\"row_heading level0 row1\" >rf</th>\n",
       "      <td id=\"T_e3ce1_row1_col0\" class=\"data row1 col0\" >Random Forest Classifier</td>\n",
       "      <td id=\"T_e3ce1_row1_col1\" class=\"data row1 col1\" >0.7683</td>\n",
       "      <td id=\"T_e3ce1_row1_col2\" class=\"data row1 col2\" >0.8541</td>\n",
       "      <td id=\"T_e3ce1_row1_col3\" class=\"data row1 col3\" >0.8254</td>\n",
       "      <td id=\"T_e3ce1_row1_col4\" class=\"data row1 col4\" >0.7720</td>\n",
       "      <td id=\"T_e3ce1_row1_col5\" class=\"data row1 col5\" >0.7978</td>\n",
       "      <td id=\"T_e3ce1_row1_col6\" class=\"data row1 col6\" >0.5273</td>\n",
       "      <td id=\"T_e3ce1_row1_col7\" class=\"data row1 col7\" >0.5290</td>\n",
       "      <td id=\"T_e3ce1_row1_col8\" class=\"data row1 col8\" >0.1380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e3ce1_level0_row2\" class=\"row_heading level0 row2\" >catboost</th>\n",
       "      <td id=\"T_e3ce1_row2_col0\" class=\"data row2 col0\" >CatBoost Classifier</td>\n",
       "      <td id=\"T_e3ce1_row2_col1\" class=\"data row2 col1\" >0.7587</td>\n",
       "      <td id=\"T_e3ce1_row2_col2\" class=\"data row2 col2\" >0.8296</td>\n",
       "      <td id=\"T_e3ce1_row2_col3\" class=\"data row2 col3\" >0.8200</td>\n",
       "      <td id=\"T_e3ce1_row2_col4\" class=\"data row2 col4\" >0.7622</td>\n",
       "      <td id=\"T_e3ce1_row2_col5\" class=\"data row2 col5\" >0.7899</td>\n",
       "      <td id=\"T_e3ce1_row2_col6\" class=\"data row2 col6\" >0.5073</td>\n",
       "      <td id=\"T_e3ce1_row2_col7\" class=\"data row2 col7\" >0.5095</td>\n",
       "      <td id=\"T_e3ce1_row2_col8\" class=\"data row2 col8\" >9.8440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e3ce1_level0_row3\" class=\"row_heading level0 row3\" >lightgbm</th>\n",
       "      <td id=\"T_e3ce1_row3_col0\" class=\"data row3 col0\" >Light Gradient Boosting Machine</td>\n",
       "      <td id=\"T_e3ce1_row3_col1\" class=\"data row3 col1\" >0.7520</td>\n",
       "      <td id=\"T_e3ce1_row3_col2\" class=\"data row3 col2\" >0.8225</td>\n",
       "      <td id=\"T_e3ce1_row3_col3\" class=\"data row3 col3\" >0.8114</td>\n",
       "      <td id=\"T_e3ce1_row3_col4\" class=\"data row3 col4\" >0.7578</td>\n",
       "      <td id=\"T_e3ce1_row3_col5\" class=\"data row3 col5\" >0.7836</td>\n",
       "      <td id=\"T_e3ce1_row3_col6\" class=\"data row3 col6\" >0.4940</td>\n",
       "      <td id=\"T_e3ce1_row3_col7\" class=\"data row3 col7\" >0.4957</td>\n",
       "      <td id=\"T_e3ce1_row3_col8\" class=\"data row3 col8\" >0.1380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e3ce1_level0_row4\" class=\"row_heading level0 row4\" >gbc</th>\n",
       "      <td id=\"T_e3ce1_row4_col0\" class=\"data row4 col0\" >Gradient Boosting Classifier</td>\n",
       "      <td id=\"T_e3ce1_row4_col1\" class=\"data row4 col1\" >0.7319</td>\n",
       "      <td id=\"T_e3ce1_row4_col2\" class=\"data row4 col2\" >0.8035</td>\n",
       "      <td id=\"T_e3ce1_row4_col3\" class=\"data row4 col3\" >0.7990</td>\n",
       "      <td id=\"T_e3ce1_row4_col4\" class=\"data row4 col4\" >0.7383</td>\n",
       "      <td id=\"T_e3ce1_row4_col5\" class=\"data row4 col5\" >0.7674</td>\n",
       "      <td id=\"T_e3ce1_row4_col6\" class=\"data row4 col6\" >0.4521</td>\n",
       "      <td id=\"T_e3ce1_row4_col7\" class=\"data row4 col7\" >0.4542</td>\n",
       "      <td id=\"T_e3ce1_row4_col8\" class=\"data row4 col8\" >0.3790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e3ce1_level0_row5\" class=\"row_heading level0 row5\" >lda</th>\n",
       "      <td id=\"T_e3ce1_row5_col0\" class=\"data row5 col0\" >Linear Discriminant Analysis</td>\n",
       "      <td id=\"T_e3ce1_row5_col1\" class=\"data row5 col1\" >0.7245</td>\n",
       "      <td id=\"T_e3ce1_row5_col2\" class=\"data row5 col2\" >0.7929</td>\n",
       "      <td id=\"T_e3ce1_row5_col3\" class=\"data row5 col3\" >0.8028</td>\n",
       "      <td id=\"T_e3ce1_row5_col4\" class=\"data row5 col4\" >0.7278</td>\n",
       "      <td id=\"T_e3ce1_row5_col5\" class=\"data row5 col5\" >0.7634</td>\n",
       "      <td id=\"T_e3ce1_row5_col6\" class=\"data row5 col6\" >0.4356</td>\n",
       "      <td id=\"T_e3ce1_row5_col7\" class=\"data row5 col7\" >0.4388</td>\n",
       "      <td id=\"T_e3ce1_row5_col8\" class=\"data row5 col8\" >0.1040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e3ce1_level0_row6\" class=\"row_heading level0 row6\" >ada</th>\n",
       "      <td id=\"T_e3ce1_row6_col0\" class=\"data row6 col0\" >Ada Boost Classifier</td>\n",
       "      <td id=\"T_e3ce1_row6_col1\" class=\"data row6 col1\" >0.7195</td>\n",
       "      <td id=\"T_e3ce1_row6_col2\" class=\"data row6 col2\" >0.7893</td>\n",
       "      <td id=\"T_e3ce1_row6_col3\" class=\"data row6 col3\" >0.7769</td>\n",
       "      <td id=\"T_e3ce1_row6_col4\" class=\"data row6 col4\" >0.7328</td>\n",
       "      <td id=\"T_e3ce1_row6_col5\" class=\"data row6 col5\" >0.7541</td>\n",
       "      <td id=\"T_e3ce1_row6_col6\" class=\"data row6 col6\" >0.4284</td>\n",
       "      <td id=\"T_e3ce1_row6_col7\" class=\"data row6 col7\" >0.4295</td>\n",
       "      <td id=\"T_e3ce1_row6_col8\" class=\"data row6 col8\" >0.1560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e3ce1_level0_row7\" class=\"row_heading level0 row7\" >lr</th>\n",
       "      <td id=\"T_e3ce1_row7_col0\" class=\"data row7 col0\" >Logistic Regression</td>\n",
       "      <td id=\"T_e3ce1_row7_col1\" class=\"data row7 col1\" >0.7099</td>\n",
       "      <td id=\"T_e3ce1_row7_col2\" class=\"data row7 col2\" >0.7778</td>\n",
       "      <td id=\"T_e3ce1_row7_col3\" class=\"data row7 col3\" >0.7833</td>\n",
       "      <td id=\"T_e3ce1_row7_col4\" class=\"data row7 col4\" >0.7182</td>\n",
       "      <td id=\"T_e3ce1_row7_col5\" class=\"data row7 col5\" >0.7492</td>\n",
       "      <td id=\"T_e3ce1_row7_col6\" class=\"data row7 col6\" >0.4066</td>\n",
       "      <td id=\"T_e3ce1_row7_col7\" class=\"data row7 col7\" >0.4089</td>\n",
       "      <td id=\"T_e3ce1_row7_col8\" class=\"data row7 col8\" >0.2980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e3ce1_level0_row8\" class=\"row_heading level0 row8\" >nb</th>\n",
       "      <td id=\"T_e3ce1_row8_col0\" class=\"data row8 col0\" >Naive Bayes</td>\n",
       "      <td id=\"T_e3ce1_row8_col1\" class=\"data row8 col1\" >0.6896</td>\n",
       "      <td id=\"T_e3ce1_row8_col2\" class=\"data row8 col2\" >0.7519</td>\n",
       "      <td id=\"T_e3ce1_row8_col3\" class=\"data row8 col3\" >0.7465</td>\n",
       "      <td id=\"T_e3ce1_row8_col4\" class=\"data row8 col4\" >0.7089</td>\n",
       "      <td id=\"T_e3ce1_row8_col5\" class=\"data row8 col5\" >0.7270</td>\n",
       "      <td id=\"T_e3ce1_row8_col6\" class=\"data row8 col6\" >0.3677</td>\n",
       "      <td id=\"T_e3ce1_row8_col7\" class=\"data row8 col7\" >0.3686</td>\n",
       "      <td id=\"T_e3ce1_row8_col8\" class=\"data row8 col8\" >0.1090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e3ce1_level0_row9\" class=\"row_heading level0 row9\" >qda</th>\n",
       "      <td id=\"T_e3ce1_row9_col0\" class=\"data row9 col0\" >Quadratic Discriminant Analysis</td>\n",
       "      <td id=\"T_e3ce1_row9_col1\" class=\"data row9 col1\" >0.6635</td>\n",
       "      <td id=\"T_e3ce1_row9_col2\" class=\"data row9 col2\" >0.7194</td>\n",
       "      <td id=\"T_e3ce1_row9_col3\" class=\"data row9 col3\" >0.7850</td>\n",
       "      <td id=\"T_e3ce1_row9_col4\" class=\"data row9 col4\" >0.6672</td>\n",
       "      <td id=\"T_e3ce1_row9_col5\" class=\"data row9 col5\" >0.7209</td>\n",
       "      <td id=\"T_e3ce1_row9_col6\" class=\"data row9 col6\" >0.3042</td>\n",
       "      <td id=\"T_e3ce1_row9_col7\" class=\"data row9 col7\" >0.3111</td>\n",
       "      <td id=\"T_e3ce1_row9_col8\" class=\"data row9 col8\" >0.1060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e3ce1_level0_row10\" class=\"row_heading level0 row10\" >dt</th>\n",
       "      <td id=\"T_e3ce1_row10_col0\" class=\"data row10 col0\" >Decision Tree Classifier</td>\n",
       "      <td id=\"T_e3ce1_row10_col1\" class=\"data row10 col1\" >0.6887</td>\n",
       "      <td id=\"T_e3ce1_row10_col2\" class=\"data row10 col2\" >0.6852</td>\n",
       "      <td id=\"T_e3ce1_row10_col3\" class=\"data row10 col3\" >0.7177</td>\n",
       "      <td id=\"T_e3ce1_row10_col4\" class=\"data row10 col4\" >0.7193</td>\n",
       "      <td id=\"T_e3ce1_row10_col5\" class=\"data row10 col5\" >0.7184</td>\n",
       "      <td id=\"T_e3ce1_row10_col6\" class=\"data row10 col6\" >0.3704</td>\n",
       "      <td id=\"T_e3ce1_row10_col7\" class=\"data row10 col7\" >0.3705</td>\n",
       "      <td id=\"T_e3ce1_row10_col8\" class=\"data row10 col8\" >0.1240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e3ce1_level0_row11\" class=\"row_heading level0 row11\" >knn</th>\n",
       "      <td id=\"T_e3ce1_row11_col0\" class=\"data row11 col0\" >K Neighbors Classifier</td>\n",
       "      <td id=\"T_e3ce1_row11_col1\" class=\"data row11 col1\" >0.6045</td>\n",
       "      <td id=\"T_e3ce1_row11_col2\" class=\"data row11 col2\" >0.6242</td>\n",
       "      <td id=\"T_e3ce1_row11_col3\" class=\"data row11 col3\" >0.6943</td>\n",
       "      <td id=\"T_e3ce1_row11_col4\" class=\"data row11 col4\" >0.6297</td>\n",
       "      <td id=\"T_e3ce1_row11_col5\" class=\"data row11 col5\" >0.6602</td>\n",
       "      <td id=\"T_e3ce1_row11_col6\" class=\"data row11 col6\" >0.1897</td>\n",
       "      <td id=\"T_e3ce1_row11_col7\" class=\"data row11 col7\" >0.1911</td>\n",
       "      <td id=\"T_e3ce1_row11_col8\" class=\"data row11 col8\" >0.1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e3ce1_level0_row12\" class=\"row_heading level0 row12\" >dummy</th>\n",
       "      <td id=\"T_e3ce1_row12_col0\" class=\"data row12 col0\" >Dummy Classifier</td>\n",
       "      <td id=\"T_e3ce1_row12_col1\" class=\"data row12 col1\" >0.5535</td>\n",
       "      <td id=\"T_e3ce1_row12_col2\" class=\"data row12 col2\" >0.5000</td>\n",
       "      <td id=\"T_e3ce1_row12_col3\" class=\"data row12 col3\" >1.0000</td>\n",
       "      <td id=\"T_e3ce1_row12_col4\" class=\"data row12 col4\" >0.5535</td>\n",
       "      <td id=\"T_e3ce1_row12_col5\" class=\"data row12 col5\" >0.7126</td>\n",
       "      <td id=\"T_e3ce1_row12_col6\" class=\"data row12 col6\" >0.0000</td>\n",
       "      <td id=\"T_e3ce1_row12_col7\" class=\"data row12 col7\" >0.0000</td>\n",
       "      <td id=\"T_e3ce1_row12_col8\" class=\"data row12 col8\" >0.0240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e3ce1_level0_row13\" class=\"row_heading level0 row13\" >svm</th>\n",
       "      <td id=\"T_e3ce1_row13_col0\" class=\"data row13 col0\" >SVM - Linear Kernel</td>\n",
       "      <td id=\"T_e3ce1_row13_col1\" class=\"data row13 col1\" >0.5705</td>\n",
       "      <td id=\"T_e3ce1_row13_col2\" class=\"data row13 col2\" >0.0000</td>\n",
       "      <td id=\"T_e3ce1_row13_col3\" class=\"data row13 col3\" >0.8084</td>\n",
       "      <td id=\"T_e3ce1_row13_col4\" class=\"data row13 col4\" >0.5891</td>\n",
       "      <td id=\"T_e3ce1_row13_col5\" class=\"data row13 col5\" >0.6726</td>\n",
       "      <td id=\"T_e3ce1_row13_col6\" class=\"data row13 col6\" >0.0847</td>\n",
       "      <td id=\"T_e3ce1_row13_col7\" class=\"data row13 col7\" >0.0882</td>\n",
       "      <td id=\"T_e3ce1_row13_col8\" class=\"data row13 col8\" >0.1130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e3ce1_level0_row14\" class=\"row_heading level0 row14\" >ridge</th>\n",
       "      <td id=\"T_e3ce1_row14_col0\" class=\"data row14 col0\" >Ridge Classifier</td>\n",
       "      <td id=\"T_e3ce1_row14_col1\" class=\"data row14 col1\" >0.7249</td>\n",
       "      <td id=\"T_e3ce1_row14_col2\" class=\"data row14 col2\" >0.0000</td>\n",
       "      <td id=\"T_e3ce1_row14_col3\" class=\"data row14 col3\" >0.8038</td>\n",
       "      <td id=\"T_e3ce1_row14_col4\" class=\"data row14 col4\" >0.7278</td>\n",
       "      <td id=\"T_e3ce1_row14_col5\" class=\"data row14 col5\" >0.7638</td>\n",
       "      <td id=\"T_e3ce1_row14_col6\" class=\"data row14 col6\" >0.4363</td>\n",
       "      <td id=\"T_e3ce1_row14_col7\" class=\"data row14 col7\" >0.4395</td>\n",
       "      <td id=\"T_e3ce1_row14_col8\" class=\"data row14 col8\" >0.0960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f5e666880a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43b619081351478496046b83b309b787",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/67 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_3 = compare_models(sort = 'AUC', n_select = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blended = blend_models(estimator_list = best_3, fold = 5, method = 'soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_holdout = predict_model(blended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = finalize_model(blended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict_model(final_model, data = test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = test_index\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"index\" : index,\n",
    "    \"nerdiness\" : predictions['Score']\n",
    "})\n",
    "submission.to_csv('./data/model_auto_ml.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_fill_na['nerdiness']\n",
    "x_train = train_fill_na.drop('nerdiness',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test_fill_na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = KFold(n_splits = 3, shuffle = True, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "params = {'bagging_temperature': 0.375906, 'depth': 9.0, 'l2_leaf_reg': 68.8, 'learning_rate': 0.011, 'subsample': 0.76046}\n",
    "\n",
    "clf1 = CatBoostClassifier(**params, iterations=5000, eval_metric='AUC',allow_writing_files=False,od_type='Iter',random_state=777)\n",
    "clf2 = LGBMClassifier()\n",
    "clf3 = GradientBoostingClassifier()\n",
    "soft_vote  = VotingClassifier([('r1',clf1), ('r2', clf2), ('r3',clf3)], voting='soft')\n",
    "soft_vote.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = soft_vote\n",
    "pred_y = model.predict_proba(test_fill_na)\n",
    "pred_y = pred_y[:,1]\n",
    "\n",
    "index = test_index\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"index\" : index,\n",
    "    \"voted\" : pred_y\n",
    "})\n",
    "submission.to_csv('./data/model1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train = pd.read_csv('./data/train.csv').drop('index',axis=1)\n",
    "# train.drop('introelapse',axis=1,inplace=True)\n",
    "# family size 이상치\n",
    "train = train.drop(1019)\n",
    "# age 이상치\n",
    "train = train.drop(train[train['age']>100].index.to_list())\n",
    "train = train.drop(train[train['introelapse']>3000].index.to_list())\n",
    "train = train.drop(train[train['testelapse']>10000].index.to_list())\n",
    "train = train.drop(train[train['surveyelapse']>10000].index.to_list()) # 나에 대한 추가 질문\n",
    "\n",
    "test = pd.read_csv('./data/test.csv').drop('index',axis=1)\n",
    "# test.drop('introelapse',axis=1,inplace=True)\n",
    "test.loc[test['familysize']>100,'familysize']= train['familysize'].mean()\n",
    "test.loc[test['age']>100,'age']= train['age'].mean()\n",
    "test.loc[test['introelapse']>3000,'introelapse']= train['introelapse'].mean()\n",
    "test.loc[test['testelapse']>10000,'testelapse']= train['testelapse'].mean()\n",
    "test.loc[test['surveyelapse']>10000,'surveyelapse']= train['surveyelapse'].mean()\n",
    "\n",
    "test_index = pd.read_csv('./data/test.csv')['index']\n",
    "\n",
    "pd.set_option('display.max_row', 100)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "value = train['country'].value_counts().values\n",
    "rank = list(map(lambda x : 1 if x>1000 else (2 if x>100 else 3),value))\n",
    "# rank = list(map(lambda x : 1 if x>2000 else (2 if x>1000 else (3 if x>500 else (4 if x>200 else (5 if x>100 else 6)))),value))\n",
    "\n",
    "temp_dict = {i : 0 for i in train['country'].value_counts().index.to_list()}\n",
    "\n",
    "rank_dict = dict(zip(train['country'].value_counts().index.to_list(), rank))\n",
    "rank_dict['nan'] = 0\n",
    "train['country'] = train['country'].fillna('nan')\n",
    "train['country'] = train['country'].apply(lambda x : rank_dict[x])\n",
    "train['Ex'] = (train['TIPI1']+train['TIPI6'])/2\n",
    "train['Ag'] = (train['TIPI7']+train['TIPI2'])/2\n",
    "train['Con'] = (train['TIPI3']+train['TIPI8'])/2\n",
    "train['Es'] =(train['TIPI9']+train['TIPI4'])/2\n",
    "train['Op'] =(train['TIPI5']+train['TIPI10'])/2\n",
    "\n",
    "train['mach_score'] = train[train.columns[:20]].apply(lambda x : x.mean(),axis=1)\n",
    "train['T'] = train['Q1'] + train['Q2'] - train['Q3'] - train['Q6'] - train['Q7'] - train['Q10'] + train['Q12'] + train['Q15'] - train['Q16']\n",
    "train['V'] = -train['Q4'] + train['Q5'] + train['Q8'] - train['Q11'] + train['Q13'] - train['Q14'] - train['Q17'] + train['Q18'] + train['Q20']\n",
    "train['M'] = -train['Q9'] + train['Q19']\n",
    "train['introelapse'] = np.log1p(train['introelapse'])\n",
    "train['testelapse'] = np.log1p(train['testelapse'])\n",
    "train['surveyelapse'] = np.log1p(train['surveyelapse'])\n",
    "train.drop('hand',axis=1,inplace=True)\n",
    "# train.drop(['VCL7','VCL8','VCL11'],axis=1,inplace=True)\n",
    "# train.drop([('TIPI'+str(i)) for i in range(1,11)],axis=1,inplace=True)\n",
    "# train.drop([('Q'+str(i)) for i in range(1,20)],axis=1,inplace=True)\n",
    "\n",
    "train['nature_score'] = train[[('Q'+str(i)) for i in range(20,27)]].apply(lambda x : x.mean(),axis=1)\n",
    "# train.drop([('Q'+str(i)) for i in range(20,27)],axis=1,inplace=True)\n",
    "train_fill_na = train.fillna(train.mean())\n",
    "\n",
    "\n",
    "value = test['country'].value_counts().values\n",
    "rank = list(map(lambda x : 1 if x>1000 else (2 if x>100 else 3),value))\n",
    "# rank = list(map(lambda x : 1 if x>2000 else (2 if x>1000 else (3 if x>500 else (4 if x>200 else (5 if x>100 else 6)))),value))\n",
    "\n",
    "temp_dict = {i : 0 for i in test['country'].value_counts().index.to_list()}\n",
    "\n",
    "rank_dict = dict(zip(test['country'].value_counts().index.to_list(), rank))\n",
    "rank_dict['nan'] = 0\n",
    "test['country'] = test['country'].fillna('nan')\n",
    "test['country'] = test['country'].apply(lambda x : rank_dict[x])\n",
    "\n",
    "test['Ex'] = (test['TIPI1']+test['TIPI6'])/2\n",
    "test['Ag'] = (test['TIPI7']+test['TIPI2'])/2\n",
    "test['Con'] = (test['TIPI3']+test['TIPI8'])/2\n",
    "test['Es'] =(test['TIPI9']+test['TIPI4'])/2\n",
    "test['Op'] =(test['TIPI5']+test['TIPI10'])/2\n",
    "\n",
    "test['mach_score'] = test[test.columns[:20]].apply(lambda x : x.mean(),axis=1)\n",
    "test['T'] = test['Q1'] + test['Q2'] - test['Q3'] - test['Q6'] - test['Q7'] - test['Q10'] + test['Q12'] + test['Q15'] - test['Q16']\n",
    "test['V'] = -test['Q4'] + test['Q5'] + test['Q8'] - test['Q11'] + test['Q13'] - test['Q14'] - test['Q17'] + test['Q18'] + test['Q20']\n",
    "test['M'] = -test['Q9'] + test['Q19']\n",
    "\n",
    "test['introelapse'] = np.log1p(test['introelapse'])\n",
    "test['testelapse'] = np.log1p(test['testelapse'])\n",
    "test['surveyelapse'] = np.log1p(test['surveyelapse'])\n",
    "test.drop('hand',axis=1,inplace=True)\n",
    "# test.drop(['VCL7','VCL8','VCL11'],axis=1,inplace=True)\n",
    "# test.drop([('TIPI'+str(i)) for i in range(1,11)],axis=1,inplace=True)\n",
    "# test.drop([('Q'+str(i)) for i in range(1,20)],axis=1,inplace=True)\n",
    "\n",
    "test['nature_score'] = test[[('Q'+str(i)) for i in range(20,27)]].apply(lambda x : x.mean(),axis=1)\n",
    "# test.drop([('Q'+str(i)) for i in range(20,27)],axis=1,inplace=True)\n",
    "test_fill_na = test.fillna(test.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_fill_na['nerdiness']\n",
    "x_train = train_fill_na.drop('nerdiness',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test_fill_na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from string import ascii_lowercase\n",
    "from itertools import combinations\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "import gc\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm_rfe_777(x_data, y_data, ratio=0.975, min_feats=30):\n",
    "    feats = x_data.columns.tolist()\n",
    "    archive = pd.DataFrame(columns=['model', 'n_feats', 'feats', 'score'])\n",
    "    while True:\n",
    "        model = LGBMClassifier(objective = 'binary', num_iterations=10**3)\n",
    "        x_train, x_val, y_train, y_val = train_test_split(x_data[feats], y_data, random_state=777)\n",
    "        model.fit(x_train, y_train, eval_set=[(x_val, y_val)], early_stopping_rounds=50, verbose=False)\n",
    "        val_pred = model.predict_proba(x_val)\n",
    "        val_pred = val_pred[:,1]\n",
    "        score = roc_auc_score(y_val, val_pred)\n",
    "        n_feats = len(feats)\n",
    "        print(n_feats, score)\n",
    "        archive = archive.append({'model': model, 'n_feats': n_feats, 'feats': feats, 'score': score}, ignore_index=True)\n",
    "        feat_imp = pd.Series(model.feature_importances_, index=feats).sort_values(ascending=False)        \n",
    "        next_n_feats = int(n_feats * ratio)\n",
    "        if next_n_feats < min_feats:\n",
    "            break\n",
    "        else:\n",
    "            feats = feat_imp.iloc[:next_n_feats].index.tolist()\n",
    "    return archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77 0.847859879109879\n",
      "75 0.8525857588357587\n",
      "73 0.8493165641603142\n",
      "71 0.8494961307461307\n",
      "69 0.8596848049973052\n",
      "67 0.8510740307615308\n",
      "65 0.8558128440940942\n",
      "63 0.8584522022022021\n",
      "61 0.8550867574305074\n",
      "59 0.8587358392045893\n",
      "57 0.8570559742434742\n",
      "55 0.8574268378955879\n",
      "53 0.856757598945099\n",
      "51 0.8526835128397628\n",
      "49 0.8449901223338723\n",
      "47 0.8506691787941788\n",
      "45 0.8528841341341341\n",
      "43 0.8561040006352506\n",
      "41 0.8539507055132054\n",
      "39 0.8468227843227842\n",
      "38 0.8514451951951951\n",
      "37 0.8468715109340109\n",
      "36 0.8549065892815894\n",
      "35 0.8482054771117271\n",
      "34 0.8481224614037115\n",
      "33 0.8503112487487487\n",
      "32 0.8472234253484254\n",
      "31 0.8476168476168476\n",
      "30 0.8451928490990991\n"
     ]
    }
   ],
   "source": [
    "lgbm_archive_4040 = lgbm_rfe_777(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LGBMClassifier(objective=\"binary\", num_iterations= 10**3)\n",
    "\n",
    "x_train_1 = x_train[lgbm_archive_4040.iloc[lgbm_archive_4040[lgbm_archive_4040['score']==lgbm_archive_4040['score'].max()].index[0],2]]\n",
    "\n",
    "model.fit(x_train_1, y_train)\n",
    "\n",
    "pred_y1 = model.predict_proba(test[lgbm_archive_4040.iloc[lgbm_archive_4040[lgbm_archive_4040['score']==lgbm_archive_4040['score'].max()].index[0],2]])\n",
    "pred_y1 = pred_y1[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm_rfe_1234(x_data, y_data, ratio=0.975, min_feats=30):\n",
    "    feats = x_data.columns.tolist()\n",
    "    archive = pd.DataFrame(columns=['model', 'n_feats', 'feats', 'score'])\n",
    "    while True:\n",
    "        model = LGBMClassifier(objective = 'binary', num_iterations=10**3)\n",
    "        x_train, x_val, y_train, y_val = train_test_split(x_data[feats], y_data, random_state=1277)\n",
    "        model.fit(x_train, y_train, eval_set=[(x_val, y_val)], early_stopping_rounds=50, verbose=False)\n",
    "        val_pred = model.predict_proba(x_val)\n",
    "        val_pred = val_pred[:,1]\n",
    "        score = roc_auc_score(y_val, val_pred)\n",
    "        n_feats = len(feats)\n",
    "        print(n_feats, score)\n",
    "        archive = archive.append({'model': model, 'n_feats': n_feats, 'feats': feats, 'score': score}, ignore_index=True)\n",
    "        feat_imp = pd.Series(model.feature_importances_, index=feats).sort_values(ascending=False)\n",
    "        next_n_feats = int(n_feats * ratio)\n",
    "        if next_n_feats < min_feats:\n",
    "            break\n",
    "        else:\n",
    "            feats = feat_imp.iloc[:next_n_feats].index.tolist()\n",
    "    return archive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77 0.851471383121075\n",
      "75 0.8509694044937102\n",
      "73 0.8521408902867136\n",
      "71 0.8495800102034592\n",
      "69 0.8503599890863655\n",
      "67 0.8529248145880937\n",
      "65 0.8523703315456541\n",
      "63 0.8529345263874141\n",
      "61 0.8531144981685671\n",
      "59 0.8503675764295844\n",
      "57 0.8484188431972336\n",
      "55 0.8490479856969475\n",
      "53 0.8427477593816739\n",
      "51 0.8517927829798287\n",
      "49 0.8504613559917704\n",
      "47 0.8505041486075253\n",
      "45 0.8481966857877832\n",
      "43 0.8489526886661178\n",
      "41 0.8465930249250295\n",
      "39 0.8468904487792117\n",
      "38 0.8491745425818396\n",
      "37 0.8476400782892423\n",
      "36 0.8484036685107957\n",
      "35 0.8412985768268577\n",
      "34 0.8482525286338746\n",
      "33 0.8411365111757013\n",
      "32 0.8461444611939261\n",
      "31 0.8349743745070124\n",
      "30 0.8461101664025765\n"
     ]
    }
   ],
   "source": [
    "lgbm_archive_1234 = lgbm_rfe_1234(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = LGBMClassifier(objective=\"binary\", num_iterations= 10**3)\n",
    "\n",
    "x_train_2 = x_train[lgbm_archive_1234.iloc[lgbm_archive_1234[lgbm_archive_1234['score']==lgbm_archive_1234['score'].max()].index[0],2]]\n",
    "\n",
    "model2.fit(x_train_2, y_train)\n",
    "\n",
    "pred_y2 = model2.predict_proba(test[lgbm_archive_1234.iloc[lgbm_archive_1234[lgbm_archive_1234['score']==lgbm_archive_1234['score'].max()].index[0],2]])\n",
    "pred_y2 = pred_y2[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm_rfe_99087(x_data, y_data, ratio=0.975, min_feats=30):\n",
    "    feats = x_data.columns.tolist()\n",
    "    archive = pd.DataFrame(columns=['model', 'n_feats', 'feats', 'score'])\n",
    "    while True:\n",
    "        model = LGBMClassifier(objective = 'binary', num_iterations=10**4)\n",
    "        x_train, x_val, y_train, y_val = train_test_split(x_data[feats], y_data, random_state=9977)\n",
    "        model.fit(x_train, y_train, eval_set=[(x_val, y_val)], early_stopping_rounds=50, verbose=False)\n",
    "        val_pred = model.predict_proba(x_val)\n",
    "        val_pred = val_pred[:,1]\n",
    "        score = roc_auc_score(y_val, val_pred)\n",
    "        n_feats = len(feats)\n",
    "        print(n_feats, score)\n",
    "        archive = archive.append({'model': model, 'n_feats': n_feats, 'feats': feats, 'score': score}, ignore_index=True)\n",
    "        feat_imp = pd.Series(model.feature_importances_, index=feats).sort_values(ascending=False)\n",
    "        next_n_feats = int(n_feats * ratio)\n",
    "        if next_n_feats < min_feats:\n",
    "            break\n",
    "        else:\n",
    "            feats = feat_imp.iloc[:next_n_feats].index.tolist()\n",
    "    return archive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77 0.8567396494774387\n",
      "75 0.8556642546573995\n",
      "73 0.8550942099571064\n",
      "71 0.8547054321475915\n",
      "69 0.8519842926441041\n",
      "67 0.8545031090018236\n",
      "65 0.8543480861390029\n",
      "63 0.8508155179106336\n",
      "61 0.857259037100511\n",
      "59 0.8532980198575744\n",
      "57 0.8467550174919497\n",
      "55 0.8492607118356904\n",
      "53 0.8476372440596948\n",
      "51 0.8527697825041441\n",
      "49 0.8522720614622928\n",
      "47 0.8488697978843652\n",
      "45 0.8461745972457198\n",
      "43 0.8501017413828038\n",
      "41 0.84628109917313\n",
      "39 0.8548036946708755\n",
      "38 0.8513269712926953\n",
      "37 0.8544234614286026\n",
      "36 0.8490925669588908\n",
      "35 0.8492970262464692\n",
      "34 0.8498371649614153\n",
      "33 0.8491914398084064\n",
      "32 0.8495994428942159\n",
      "31 0.8467336560738445\n",
      "30 0.8448025838771341\n"
     ]
    }
   ],
   "source": [
    "lgbm_archive_99087 = lgbm_rfe_99087(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = LGBMClassifier(objective=\"binary\", num_iterations= 10**3)\n",
    "\n",
    "x_train_3 = x_train[lgbm_archive_99087.iloc[lgbm_archive_99087[lgbm_archive_99087['score']==lgbm_archive_99087['score'].max()].index[0],2]]\n",
    "\n",
    "model3.fit(x_train_3, y_train)\n",
    "\n",
    "pred_y3 = model3.predict_proba(test[lgbm_archive_99087.iloc[lgbm_archive_99087[lgbm_archive_99087['score']==lgbm_archive_99087['score'].max()].index[0],2]])\n",
    "pred_y3 = pred_y3[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm_rfe_42(x_data, y_data, ratio=0.975, min_feats=30):\n",
    "    feats = x_data.columns.tolist()\n",
    "    archive = pd.DataFrame(columns=['model', 'n_feats', 'feats', 'score'])\n",
    "    while True:\n",
    "        model = LGBMClassifier(objective = 'binary', num_iterations=10**3)\n",
    "        x_train, x_val, y_train, y_val = train_test_split(x_data[feats], y_data, random_state=42)\n",
    "        model.fit(x_train, y_train, eval_set=[(x_val, y_val)], early_stopping_rounds=50, verbose=False)\n",
    "        val_pred = model.predict_proba(x_val)\n",
    "        val_pred = val_pred[:,1]\n",
    "        score = roc_auc_score(y_val, val_pred)\n",
    "        n_feats = len(feats)\n",
    "        print(n_feats, score)\n",
    "        archive = archive.append({'model': model, 'n_feats': n_feats, 'feats': feats, 'score': score}, ignore_index=True)\n",
    "        feat_imp = pd.Series(model.feature_importances_, index=feats).sort_values(ascending=False)\n",
    "        next_n_feats = int(n_feats * ratio)\n",
    "        if next_n_feats < min_feats:\n",
    "            break\n",
    "        else:\n",
    "            feats = feat_imp.iloc[:next_n_feats].index.tolist()\n",
    "    return archive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77 0.8519725097102603\n",
      "75 0.8544796868591831\n",
      "73 0.847738293406673\n",
      "71 0.8567299341391039\n",
      "69 0.8512858555885262\n",
      "67 0.8524073604979374\n",
      "65 0.851249366722154\n",
      "63 0.8531018552025283\n",
      "61 0.8567938650454754\n",
      "59 0.8471457262792212\n",
      "57 0.8491037610672842\n",
      "55 0.8542293913295216\n",
      "53 0.8533587850715303\n",
      "51 0.8504203758654797\n",
      "49 0.8510554630768861\n",
      "47 0.8542839738486405\n",
      "45 0.8513135991894043\n",
      "43 0.8452410677667608\n",
      "41 0.8529221249185786\n",
      "39 0.8453342500784059\n",
      "38 0.8429247786543146\n",
      "37 0.84436352560855\n",
      "36 0.8482419000747871\n",
      "35 0.8460013027429977\n",
      "34 0.8479355142216112\n",
      "33 0.8519655738100407\n",
      "32 0.8425499384815806\n",
      "31 0.8481891269209427\n",
      "30 0.8478338882053509\n"
     ]
    }
   ],
   "source": [
    "lgbm_archive_42 = lgbm_rfe_42(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = LGBMClassifier(objective=\"binary\", num_iterations= 10**3)\n",
    "\n",
    "x_train_4 = x_train[lgbm_archive_42.iloc[lgbm_archive_42[lgbm_archive_42['score']==lgbm_archive_42['score'].max()].index[0],2]]\n",
    "\n",
    "model4.fit(x_train_4, y_train)\n",
    "\n",
    "pred_y4 = model4.predict_proba(test[lgbm_archive_42.iloc[lgbm_archive_42[lgbm_archive_42['score']==lgbm_archive_42['score'].max()].index[0],2]])\n",
    "pred_y4 = pred_y4[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_all = (pred_y1 + pred_y2 + pred_y3 + pred_y4) * (1/4)\n",
    "\n",
    "index = pd.read_csv('./data/test.csv').index.to_list()\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"nerdiness\" : index,\n",
    "    \"voted\" : pred_all\n",
    "})\n",
    "submission.to_csv('./data/model2_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train = pd.read_csv('./data/train.csv').drop('index',axis=1)\n",
    "# train.drop('introelapse',axis=1,inplace=True)\n",
    "# family size 이상치\n",
    "train = train.drop(1019)\n",
    "# age 이상치\n",
    "train = train.drop(train[train['age']>100].index.to_list())\n",
    "train = train.drop(train[train['introelapse']>3000].index.to_list())\n",
    "train = train.drop(train[train['testelapse']>10000].index.to_list())\n",
    "train = train.drop(train[train['surveyelapse']>10000].index.to_list()) # 나에 대한 추가 질문\n",
    "\n",
    "test = pd.read_csv('./data/test.csv').drop('index',axis=1)\n",
    "# test.drop('introelapse',axis=1,inplace=True)\n",
    "test.loc[test['familysize']>100,'familysize']= train['familysize'].mean()\n",
    "test.loc[test['age']>100,'age']= train['age'].mean()\n",
    "test.loc[test['introelapse']>3000,'introelapse']= train['introelapse'].mean()\n",
    "test.loc[test['testelapse']>10000,'testelapse']= train['testelapse'].mean()\n",
    "test.loc[test['surveyelapse']>10000,'surveyelapse']= train['surveyelapse'].mean()\n",
    "\n",
    "test_index = pd.read_csv('./data/test.csv')['index']\n",
    "\n",
    "pd.set_option('display.max_row', 100)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "value = train['country'].value_counts().values\n",
    "rank = list(map(lambda x : 1 if x>1000 else (2 if x>100 else 3),value))\n",
    "# rank = list(map(lambda x : 1 if x>2000 else (2 if x>1000 else (3 if x>500 else (4 if x>200 else (5 if x>100 else 6)))),value))\n",
    "\n",
    "temp_dict = {i : 0 for i in train['country'].value_counts().index.to_list()}\n",
    "\n",
    "rank_dict = dict(zip(train['country'].value_counts().index.to_list(), rank))\n",
    "rank_dict['nan'] = 0\n",
    "train['country'] = train['country'].fillna('nan')\n",
    "train['country'] = train['country'].apply(lambda x : rank_dict[x])\n",
    "train['Ex'] = (train['TIPI1']+train['TIPI6'])/2\n",
    "train['Ag'] = (train['TIPI7']+train['TIPI2'])/2\n",
    "train['Con'] = (train['TIPI3']+train['TIPI8'])/2\n",
    "train['Es'] =(train['TIPI9']+train['TIPI4'])/2\n",
    "train['Op'] =(train['TIPI5']+train['TIPI10'])/2\n",
    "\n",
    "train['mach_score'] = train[train.columns[:20]].apply(lambda x : x.mean(),axis=1)\n",
    "train['T'] = train['Q1'] + train['Q2'] - train['Q3'] - train['Q6'] - train['Q7'] - train['Q10'] + train['Q12'] + train['Q15'] - train['Q16']\n",
    "train['V'] = -train['Q4'] + train['Q5'] + train['Q8'] - train['Q11'] + train['Q13'] - train['Q14'] - train['Q17'] + train['Q18'] + train['Q20']\n",
    "train['M'] = -train['Q9'] + train['Q19']\n",
    "train['introelapse'] = np.log1p(train['introelapse'])\n",
    "train['testelapse'] = np.log1p(train['testelapse'])\n",
    "train['surveyelapse'] = np.log1p(train['surveyelapse'])\n",
    "train.drop('hand',axis=1,inplace=True)\n",
    "# train.drop(['VCL7','VCL8','VCL11'],axis=1,inplace=True)\n",
    "# train.drop([('TIPI'+str(i)) for i in range(1,11)],axis=1,inplace=True)\n",
    "# train.drop([('Q'+str(i)) for i in range(1,20)],axis=1,inplace=True)\n",
    "\n",
    "train['nature_score'] = train[[('Q'+str(i)) for i in range(20,27)]].apply(lambda x : x.mean(),axis=1)\n",
    "# train.drop([('Q'+str(i)) for i in range(20,27)],axis=1,inplace=True)\n",
    "train_fill_na = train.fillna(train.mean())\n",
    "\n",
    "\n",
    "value = test['country'].value_counts().values\n",
    "rank = list(map(lambda x : 1 if x>1000 else (2 if x>100 else 3),value))\n",
    "# rank = list(map(lambda x : 1 if x>2000 else (2 if x>1000 else (3 if x>500 else (4 if x>200 else (5 if x>100 else 6)))),value))\n",
    "\n",
    "temp_dict = {i : 0 for i in test['country'].value_counts().index.to_list()}\n",
    "\n",
    "rank_dict = dict(zip(test['country'].value_counts().index.to_list(), rank))\n",
    "rank_dict['nan'] = 0\n",
    "test['country'] = test['country'].fillna('nan')\n",
    "test['country'] = test['country'].apply(lambda x : rank_dict[x])\n",
    "\n",
    "test['Ex'] = (test['TIPI1']+test['TIPI6'])/2\n",
    "test['Ag'] = (test['TIPI7']+test['TIPI2'])/2\n",
    "test['Con'] = (test['TIPI3']+test['TIPI8'])/2\n",
    "test['Es'] =(test['TIPI9']+test['TIPI4'])/2\n",
    "test['Op'] =(test['TIPI5']+test['TIPI10'])/2\n",
    "\n",
    "test['mach_score'] = test[test.columns[:20]].apply(lambda x : x.mean(),axis=1)\n",
    "test['T'] = test['Q1'] + test['Q2'] - test['Q3'] - test['Q6'] - test['Q7'] - test['Q10'] + test['Q12'] + test['Q15'] - test['Q16']\n",
    "test['V'] = -test['Q4'] + test['Q5'] + test['Q8'] - test['Q11'] + test['Q13'] - test['Q14'] - test['Q17'] + test['Q18'] + test['Q20']\n",
    "test['M'] = -test['Q9'] + test['Q19']\n",
    "\n",
    "test['introelapse'] = np.log1p(test['introelapse'])\n",
    "test['testelapse'] = np.log1p(test['testelapse'])\n",
    "test['surveyelapse'] = np.log1p(test['surveyelapse'])\n",
    "test.drop('hand',axis=1,inplace=True)\n",
    "# test.drop(['VCL7','VCL8','VCL11'],axis=1,inplace=True)\n",
    "# test.drop([('TIPI'+str(i)) for i in range(1,11)],axis=1,inplace=True)\n",
    "# test.drop([('Q'+str(i)) for i in range(1,20)],axis=1,inplace=True)\n",
    "\n",
    "test['nature_score'] = test[[('Q'+str(i)) for i in range(20,27)]].apply(lambda x : x.mean(),axis=1)\n",
    "# test.drop([('Q'+str(i)) for i in range(20,27)],axis=1,inplace=True)\n",
    "test_fill_na = test.fillna(test.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ss = StandardScaler()\n",
    "\n",
    "y_train = train_fill_na['nerdiness']\n",
    "train_x = train_fill_na.drop('nerdiness',axis=1)\n",
    "test_x = test_fill_na.copy()\n",
    "\n",
    "ss.fit(train_x)\n",
    "train_x = ss.transform(train_x)\n",
    "test_x = ss.transform(test_x)\n",
    "# train_fill_na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/07: 100%|██████████| 48/48 [00:38<00:00,  1.25it/s]\n",
      "02/07: 100%|██████████| 48/48 [00:38<00:00,  1.24it/s]\n",
      "03/07: 100%|██████████| 48/48 [00:38<00:00,  1.23it/s]\n",
      "04/07: 100%|██████████| 48/48 [00:39<00:00,  1.22it/s]\n",
      "05/07: 100%|██████████| 48/48 [00:39<00:00,  1.21it/s]\n",
      "06/07: 100%|██████████| 48/48 [00:39<00:00,  1.22it/s]\n",
      "07/07: 100%|██████████| 48/48 [00:39<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R1 -> 0.6530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/07: 100%|██████████| 48/48 [00:39<00:00,  1.21it/s]\n",
      "02/07: 100%|██████████| 48/48 [00:39<00:00,  1.20it/s]\n",
      "03/07: 100%|██████████| 48/48 [00:39<00:00,  1.23it/s]\n",
      "04/07: 100%|██████████| 48/48 [00:39<00:00,  1.23it/s]\n",
      "05/07: 100%|██████████| 48/48 [00:39<00:00,  1.22it/s]\n",
      "06/07: 100%|██████████| 48/48 [00:39<00:00,  1.23it/s]\n",
      "07/07: 100%|██████████| 48/48 [00:39<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 -> 0.6536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/07: 100%|██████████| 48/48 [00:38<00:00,  1.23it/s]\n",
      "02/07: 100%|██████████| 48/48 [00:39<00:00,  1.22it/s]\n",
      "03/07: 100%|██████████| 48/48 [00:40<00:00,  1.20it/s]\n",
      "04/07: 100%|██████████| 48/48 [00:39<00:00,  1.22it/s]\n",
      "05/07: 100%|██████████| 48/48 [00:39<00:00,  1.21it/s]\n",
      "06/07: 100%|██████████| 48/48 [00:39<00:00,  1.21it/s]\n",
      "07/07: 100%|██████████| 48/48 [00:40<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R3 -> 0.6527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/07: 100%|██████████| 48/48 [00:40<00:00,  1.19it/s]\n",
      "02/07: 100%|██████████| 48/48 [00:40<00:00,  1.19it/s]\n",
      "03/07: 100%|██████████| 48/48 [00:39<00:00,  1.21it/s]\n",
      "04/07: 100%|██████████| 48/48 [00:39<00:00,  1.20it/s]\n",
      "05/07: 100%|██████████| 48/48 [00:40<00:00,  1.20it/s]\n",
      "06/07: 100%|██████████| 48/48 [00:39<00:00,  1.21it/s]\n",
      "07/07: 100%|██████████| 48/48 [00:39<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R4 -> 0.6489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/07: 100%|██████████| 48/48 [00:39<00:00,  1.21it/s]\n",
      "02/07: 100%|██████████| 48/48 [00:40<00:00,  1.18it/s]\n",
      "03/07: 100%|██████████| 48/48 [00:40<00:00,  1.19it/s]\n",
      "04/07: 100%|██████████| 48/48 [00:40<00:00,  1.18it/s]\n",
      "05/07: 100%|██████████| 48/48 [00:40<00:00,  1.19it/s]\n",
      "06/07: 100%|██████████| 48/48 [00:39<00:00,  1.22it/s]\n",
      "07/07: 100%|██████████| 48/48 [00:39<00:00,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R5 -> 0.6569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print('Used',DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "train_y = y_train.copy()\n",
    "\n",
    "# train_y = 2 - train_y.to_numpy()\n",
    "# train_x = train_x.to_numpy()\n",
    "# test_x = test_x.to_numpy()\n",
    "\n",
    "train_y_t = torch.tensor(train_y, dtype=torch.float32)\n",
    "train_x_t = torch.tensor(train_x, dtype=torch.float32)\n",
    "test_x_t = torch.tensor(test_x, dtype=torch.float32)\n",
    "\n",
    "train_x_t[:, :26] = (train_x_t[:, :26] - 3.) / 2.\n",
    "test_x_t[:, :26] = (test_x_t[:, :26] - 3.) / 2\n",
    "\n",
    "train_x_t[:, 30:40] = (train_x_t[:, 30:40] - 3.) / 2.\n",
    "test_x_t[:, 30:40] = (test_x_t[:, 30:40] - 3.) / 2\n",
    "\n",
    "test_len = len(test_x_t)\n",
    "\n",
    "N_REPEAT = 5\n",
    "N_SKFOLD = 7\n",
    "N_EPOCH = 48\n",
    "BATCH_SIZE = 1024\n",
    "LOADER_PARAM = {\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'num_workers': 4, # in colab use 2\n",
    "    'pin_memory': True\n",
    "}\n",
    "prediction = np.zeros((test_len, 1), dtype=np.float32)\n",
    "\n",
    "for repeat in range(N_REPEAT):\n",
    "\n",
    "    skf, tot = StratifiedKFold(n_splits=N_SKFOLD, random_state=repeat, shuffle=True), 0.\n",
    "    for skfold, (train_idx, valid_idx) in enumerate(skf.split(train_x, train_y)):\n",
    "        train_idx, valid_idx = list(train_idx), list(valid_idx)\n",
    "        train_loader = DataLoader(TensorDataset(train_x_t[train_idx, :], train_y_t[train_idx]),\n",
    "                                  shuffle=True, drop_last=True, **LOADER_PARAM)\n",
    "        valid_loader = DataLoader(TensorDataset(train_x_t[valid_idx, :], train_y_t[valid_idx]),\n",
    "                                  shuffle=False, drop_last=False, **LOADER_PARAM)\n",
    "        test_loader = DataLoader(TensorDataset(test_x_t, torch.zeros((test_len,), dtype=torch.float32)),\n",
    "                                 shuffle=False, drop_last=False, **LOADER_PARAM)\n",
    "        model = nn.Sequential(\n",
    "            nn.Dropout(0.05),\n",
    "            nn.Linear(77, 180, bias=False),\n",
    "            nn.LeakyReLU(0.5, inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(180, 32, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.Dropout(0.5),\n",
    "            # nn.Linear(128, 32, bias=False),\n",
    "            # nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        ).to(DEVICE)\n",
    "        # criterion = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor([1.20665], device=DEVICE))\n",
    "        criterion = torch.nn.BCELoss()\n",
    "\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=1e-5, weight_decay=4e-2)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            optimizer, T_0=N_EPOCH // 4, eta_min=1.2e-5)\n",
    "        prediction_t, loss_t = np.zeros((test_len, 1), dtype=np.float32), 1.\n",
    "\n",
    "        # for epoch in range(N_EPOCH):\n",
    "        for epoch in tqdm(range(N_EPOCH), desc='{:02d}/{:02d}'.format(skfold + 1, N_SKFOLD)):\n",
    "            model.train()\n",
    "            for idx, (xx, yy) in enumerate(train_loader):\n",
    "                optimizer.zero_grad()\n",
    "                xx, yy = xx.to(DEVICE), yy.to(DEVICE)\n",
    "                pred = model(xx).squeeze()\n",
    "                loss = criterion(pred, yy)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step(epoch + idx / len(train_loader))\n",
    "\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                running_acc, running_loss, running_count = 0, 0., 0\n",
    "                for xx, yy in valid_loader:\n",
    "                    xx, yy = xx.to(DEVICE), yy.to(DEVICE)\n",
    "                    pred = model(xx).squeeze()\n",
    "                    loss = criterion(pred, yy)\n",
    "                    running_loss += loss.item() * len(yy)\n",
    "                    running_count += len(yy)\n",
    "                    # running_acc += ((torch.sigmoid(pred) > 0.5).float() == yy).sum().item()\n",
    "                    running_acc += (((pred) >\n",
    "                                    0.5).float() == yy).sum().item()\n",
    "                # print('R{:02d} S{:02d} E{:02d} | {:6.4f}, {:5.2f}%'.format(repeat + 1, skfold + 1, epoch + 1, running_loss / running_count,running_acc / running_count * 100))\n",
    "\n",
    "                if running_loss / running_count < loss_t:\n",
    "                    loss_t = running_loss / running_count\n",
    "                    for idx, (xx, _) in enumerate(test_loader):\n",
    "                        xx = xx.to(DEVICE)\n",
    "                        pred = ((model(xx).detach().to('cpu'))).numpy()\n",
    "                        prediction_t[BATCH_SIZE * idx:min(BATCH_SIZE * (idx + 1), len(prediction)), :] \\\n",
    "                            = pred[:, :].copy()\n",
    "        prediction[:, :] += prediction_t[:, :].copy() / (N_REPEAT * N_SKFOLD)\n",
    "        tot += loss_t\n",
    "    print('R{} -> {:6.4f}'.format(repeat + 1, tot / N_SKFOLD))\n",
    "\n",
    "df = pd.read_csv('./data/sample_submission.csv')\n",
    "df.iloc[:, 1:] = prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df['nerdiness']:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./data/model3.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# final Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = pd.read_csv('./data/model1.csv', index_col = 'index')\n",
    "model2 = pd.read_csv('./data/model2.csv', index_col='index')\n",
    "\n",
    "pred_y = (model1)*(0.7) + (model2)*(0.3)\n",
    "\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "index = test['index']\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'index': index,\n",
    "    'voted': pred_y['voted']\n",
    "    })\n",
    "\n",
    "submission.to_csv('./data/combined_model1_model2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_12 = pd.read_csv('./data/combined_model1_model2.csv', index_col = 'index')\n",
    "model3 = pd.read_csv('./data/model3.csv', index_col='index')\n",
    "model3['voted'] = model3['voted']-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = (model3)*(0.8) + (combined_12)*(0.2)\n",
    "\n",
    "test = pd.read_csv('./data/test_x.csv')\n",
    "index = test['index']\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'index': index,\n",
    "    'voted': pred_y['voted']\n",
    "    })\n",
    "\n",
    "submission.to_csv('./data/submission_final_fix_lgbm.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('for_pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6d5361de2d3a86ff8022af11ead2fa25aee948dfb14ed55cb3d2da795443f4cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
