{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from string import ascii_lowercase\n",
    "from itertools import combinations\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import  GradientBoostingClassifier\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "train = pd.read_csv('./data/train.csv').drop('index',axis=1)\n",
    "# train.drop('introelapse',axis=1,inplace=True)\n",
    "# family size 이상치\n",
    "train = train.drop(1019)\n",
    "# age 이상치\n",
    "train = train.drop(train[train['age']>100].index.to_list())\n",
    "train = train.drop(train[train['familysize']>100].index.to_list())\n",
    "# train = train.drop(train[train['introelapse']>3000].index.to_list())\n",
    "# train = train.drop(train[train['testelapse']>10000].index.to_list())\n",
    "# train = train.drop(train[train['surveyelapse']>10000].index.to_list()) # 나에 대한 추가 질문\n",
    "\n",
    "test = pd.read_csv('./data/test.csv').drop('index',axis=1)\n",
    "# test.drop('introelapse',axis=1,inplace=True)\n",
    "test.loc[test['familysize']>100,'familysize']= train['familysize'].mean()\n",
    "test.loc[test['age']>100,'age']= train['age'].mean()\n",
    "# test.loc[test['introelapse']>3000,'introelapse']= train['introelapse'].mean()\n",
    "# test.loc[test['testelapse']>10000,'testelapse']= train['testelapse'].mean()\n",
    "# test.loc[test['surveyelapse']>10000,'surveyelapse']= train['surveyelapse'].mean()\n",
    "\n",
    "test_index = pd.read_csv('./data/test.csv')['index']\n",
    "\n",
    "pd.set_option('display.max_row', 100)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "value = train['country'].value_counts().values\n",
    "rank = list(map(lambda x : 1 if x>1000 else (2 if x>100 else 3),value))\n",
    "# rank = list(map(lambda x : 1 if x>1000 else (2 if x>500 else (3 if x>200 else (4 if x>100 else 5))),value))\n",
    "temp_dict = {i : 0 for i in train['country'].value_counts().index.to_list()}\n",
    "\n",
    "rank_dict = dict(zip(train['country'].value_counts().index.to_list(), rank))\n",
    "rank_dict['nan'] = 0\n",
    "train['country'] = train['country'].fillna('nan')\n",
    "train['country'] = train['country'].apply(lambda x : rank_dict[x])\n",
    "\n",
    "\n",
    "train['Ex'] = (train['TIPI1']+train['TIPI6'])/2\n",
    "train['Ag'] = (train['TIPI7']+train['TIPI2'])/2\n",
    "train['Con'] = (train['TIPI3']+train['TIPI8'])/2\n",
    "train['Es'] =(train['TIPI9']+train['TIPI4'])/2\n",
    "train['Op'] =(train['TIPI5']+train['TIPI10'])/2\n",
    "\n",
    "train['mach_score'] = train[train.columns[:20]].apply(lambda x : x.mean(),axis=1)\n",
    "train['T'] = train['Q1'] + train['Q2'] - train['Q3'] - train['Q6'] - train['Q7'] - train['Q10'] + train['Q12'] + train['Q15'] - train['Q16']\n",
    "train['V'] = -train['Q4'] + train['Q5'] + train['Q8'] - train['Q11'] + train['Q13'] - train['Q14'] - train['Q17'] + train['Q18'] + train['Q20']\n",
    "train['M'] = -train['Q9'] + train['Q19']\n",
    "train['introelapse'] = np.log1p(train['introelapse'])\n",
    "train['testelapse'] = np.log1p(train['testelapse'])\n",
    "train['surveyelapse'] = np.log1p(train['surveyelapse'])\n",
    "train.drop('hand',axis=1,inplace=True)\n",
    "train.drop(['VCL7','VCL8','VCL11'],axis=1,inplace=True)\n",
    "\n",
    "# train.drop(['country'],axis=1,inplace=True)\n",
    "\n",
    "## 6,7,8,9,11,12 의미 없어 보임\n",
    "# 6,8,10 실존하지 않는 단어 -> 얘네는 의미있게 작용하지 않을까?\n",
    "# -> 7,8,11 제거\n",
    "\n",
    "# train.drop([('TIPI'+str(i)) for i in range(1,11)],axis=1,inplace=True)\n",
    "# train.drop([('Q'+str(i)) for i in range(1,20)],axis=1,inplace=True)\n",
    "\n",
    "train['nature_score'] = train[[('Q'+str(i)) for i in range(20,27)]].apply(lambda x : x.mean(),axis=1)\n",
    "# train.drop([('Q'+str(i)) for i in range(20,27)],axis=1,inplace=True)\n",
    "# train.drop(['country','hand','introelapse','testelapse','surveyelapse'],axis=1,inplace=True)\n",
    "# train.drop(['introelapse','testelapse','surveyelapse'],axis=1,inplace=True)\n",
    "train_fill_na = train.fillna(train.mean())\n",
    "\n",
    "\n",
    "value = test['country'].value_counts().values\n",
    "# rank = list(map(lambda x : 1 if x>1000 else (2 if x>100 else 3),value))\n",
    "rank = list(map(lambda x : 1 if x>1000 else (2 if x>500 else (3 if x>200 else (4 if x>100 else 5))),value))\n",
    "\n",
    "temp_dict = {i : 0 for i in test['country'].value_counts().index.to_list()}\n",
    "\n",
    "rank_dict = dict(zip(test['country'].value_counts().index.to_list(), rank))\n",
    "rank_dict['nan'] = 0\n",
    "test['country'] = test['country'].fillna('nan')\n",
    "test['country'] = test['country'].apply(lambda x : rank_dict[x])\n",
    "\n",
    "test['Ex'] = (test['TIPI1']+test['TIPI6'])/2\n",
    "test['Ag'] = (test['TIPI7']+test['TIPI2'])/2\n",
    "test['Con'] = (test['TIPI3']+test['TIPI8'])/2\n",
    "test['Es'] =(test['TIPI9']+test['TIPI4'])/2\n",
    "test['Op'] =(test['TIPI5']+test['TIPI10'])/2\n",
    "\n",
    "test['mach_score'] = test[test.columns[:20]].apply(lambda x : x.mean(),axis=1)\n",
    "test['T'] = test['Q1'] + test['Q2'] - test['Q3'] - test['Q6'] - test['Q7'] - test['Q10'] + test['Q12'] + test['Q15'] - test['Q16']\n",
    "test['V'] = -test['Q4'] + test['Q5'] + test['Q8'] - test['Q11'] + test['Q13'] - test['Q14'] - test['Q17'] + test['Q18'] + test['Q20']\n",
    "test['M'] = -test['Q9'] + test['Q19']\n",
    "\n",
    "test['introelapse'] = np.log1p(test['introelapse'])\n",
    "test['testelapse'] = np.log1p(test['testelapse'])\n",
    "test['surveyelapse'] = np.log1p(test['surveyelapse'])\n",
    "test.drop('hand',axis=1,inplace=True)\n",
    "test.drop(['VCL7','VCL8','VCL11'],axis=1,inplace=True)\n",
    "# test.drop([('TIPI'+str(i)) for i in range(1,11)],axis=1,inplace=True)\n",
    "# test.drop([('Q'+str(i)) for i in range(1,20)],axis=1,inplace=True)\n",
    "\n",
    "test['nature_score'] = test[[('Q'+str(i)) for i in range(20,27)]].apply(lambda x : x.mean(),axis=1)\n",
    "# test.drop([('Q'+str(i)) for i in range(20,27)],axis=1,inplace=True)\n",
    "# test.drop(['country','hand','introelapse','testelapse','surveyelapse'],axis=1,inplace=True)\n",
    "# test.drop(['country'],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "test_fill_na = test.fillna(test.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q1</th>\n",
       "      <th>Q2</th>\n",
       "      <th>Q3</th>\n",
       "      <th>Q4</th>\n",
       "      <th>Q5</th>\n",
       "      <th>Q6</th>\n",
       "      <th>Q7</th>\n",
       "      <th>Q8</th>\n",
       "      <th>Q9</th>\n",
       "      <th>Q10</th>\n",
       "      <th>...</th>\n",
       "      <th>Ex</th>\n",
       "      <th>Ag</th>\n",
       "      <th>Con</th>\n",
       "      <th>Es</th>\n",
       "      <th>Op</th>\n",
       "      <th>mach_score</th>\n",
       "      <th>T</th>\n",
       "      <th>V</th>\n",
       "      <th>M</th>\n",
       "      <th>nature_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.10</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.40</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.20</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.00</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.25</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>3.714286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Q1   Q2   Q3   Q4   Q5   Q6   Q7   Q8   Q9  Q10  ...   Ex   Ag  Con   Es  \\\n",
       "0  4.0  4.0  3.0  5.0  5.0  5.0  3.0  5.0  4.0  5.0  ...  3.0  3.5  3.0  3.5   \n",
       "1  4.0  5.0  4.0  4.0  5.0  4.0  5.0  5.0  5.0  4.0  ...  3.0  3.5  3.0  4.0   \n",
       "2  5.0  5.0  5.0  5.0  4.0  5.0  5.0  5.0  5.0  4.0  ...  3.5  4.0  3.5  2.5   \n",
       "3  5.0  4.0  3.0  4.0  5.0  4.0  5.0  4.0  4.0  5.0  ...  2.5  4.0  3.5  3.0   \n",
       "4  5.0  5.0  5.0  5.0  5.0  3.0  5.0  5.0  5.0  5.0  ...  3.5  3.0  3.5  4.0   \n",
       "\n",
       "    Op  mach_score    T     V    M  nature_score  \n",
       "0  2.0        4.10 -1.0   5.0  0.0      4.285714  \n",
       "1  2.5        4.40 -4.0   4.0  0.0      4.000000  \n",
       "2  2.5        4.20 -3.0   8.0 -3.0      4.000000  \n",
       "3  2.5        4.00 -4.0   7.0  1.0      3.142857  \n",
       "4  4.5        4.25 -5.0  10.0 -4.0      3.714286  \n",
       "\n",
       "[5 rows x 74 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_4a3b0_row9_col1, #T_4a3b0_row14_col1 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_4a3b0\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_4a3b0_level0_col0\" class=\"col_heading level0 col0\" >Description</th>\n",
       "      <th id=\"T_4a3b0_level0_col1\" class=\"col_heading level0 col1\" >Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_4a3b0_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_4a3b0_row0_col0\" class=\"data row0 col0\" >Session id</td>\n",
       "      <td id=\"T_4a3b0_row0_col1\" class=\"data row0 col1\" >2443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4a3b0_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_4a3b0_row1_col0\" class=\"data row1 col0\" >Target</td>\n",
       "      <td id=\"T_4a3b0_row1_col1\" class=\"data row1 col1\" >nerdiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4a3b0_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_4a3b0_row2_col0\" class=\"data row2 col0\" >Target type</td>\n",
       "      <td id=\"T_4a3b0_row2_col1\" class=\"data row2 col1\" >Binary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4a3b0_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_4a3b0_row3_col0\" class=\"data row3 col0\" >Original data shape</td>\n",
       "      <td id=\"T_4a3b0_row3_col1\" class=\"data row3 col1\" >(14993, 75)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4a3b0_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_4a3b0_row4_col0\" class=\"data row4 col0\" >Transformed data shape</td>\n",
       "      <td id=\"T_4a3b0_row4_col1\" class=\"data row4 col1\" >(14495, 75)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4a3b0_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_4a3b0_row5_col0\" class=\"data row5 col0\" >Transformed train set shape</td>\n",
       "      <td id=\"T_4a3b0_row5_col1\" class=\"data row5 col1\" >(11487, 75)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4a3b0_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_4a3b0_row6_col0\" class=\"data row6 col0\" >Transformed test set shape</td>\n",
       "      <td id=\"T_4a3b0_row6_col1\" class=\"data row6 col1\" >(2999, 75)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4a3b0_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_4a3b0_row7_col0\" class=\"data row7 col0\" >Numeric features</td>\n",
       "      <td id=\"T_4a3b0_row7_col1\" class=\"data row7 col1\" >74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4a3b0_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_4a3b0_row8_col0\" class=\"data row8 col0\" >Rows with missing values</td>\n",
       "      <td id=\"T_4a3b0_row8_col1\" class=\"data row8 col1\" >14.6%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4a3b0_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_4a3b0_row9_col0\" class=\"data row9 col0\" >Preprocess</td>\n",
       "      <td id=\"T_4a3b0_row9_col1\" class=\"data row9 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4a3b0_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_4a3b0_row10_col0\" class=\"data row10 col0\" >Imputation type</td>\n",
       "      <td id=\"T_4a3b0_row10_col1\" class=\"data row10 col1\" >simple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4a3b0_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_4a3b0_row11_col0\" class=\"data row11 col0\" >Numeric imputation</td>\n",
       "      <td id=\"T_4a3b0_row11_col1\" class=\"data row11 col1\" >mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4a3b0_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_4a3b0_row12_col0\" class=\"data row12 col0\" >Categorical imputation</td>\n",
       "      <td id=\"T_4a3b0_row12_col1\" class=\"data row12 col1\" >constant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4a3b0_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_4a3b0_row13_col0\" class=\"data row13 col0\" >Low variance threshold</td>\n",
       "      <td id=\"T_4a3b0_row13_col1\" class=\"data row13 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4a3b0_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_4a3b0_row14_col0\" class=\"data row14 col0\" >Remove outliers</td>\n",
       "      <td id=\"T_4a3b0_row14_col1\" class=\"data row14 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4a3b0_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_4a3b0_row15_col0\" class=\"data row15 col0\" >Outliers threshold</td>\n",
       "      <td id=\"T_4a3b0_row15_col1\" class=\"data row15 col1\" >0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4a3b0_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_4a3b0_row16_col0\" class=\"data row16 col0\" >Fold Generator</td>\n",
       "      <td id=\"T_4a3b0_row16_col1\" class=\"data row16 col1\" >StratifiedKFold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4a3b0_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_4a3b0_row17_col0\" class=\"data row17 col0\" >Fold Number</td>\n",
       "      <td id=\"T_4a3b0_row17_col1\" class=\"data row17 col1\" >10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4a3b0_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_4a3b0_row18_col0\" class=\"data row18 col0\" >CPU Jobs</td>\n",
       "      <td id=\"T_4a3b0_row18_col1\" class=\"data row18 col1\" >-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4a3b0_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_4a3b0_row19_col0\" class=\"data row19 col0\" >Use GPU</td>\n",
       "      <td id=\"T_4a3b0_row19_col1\" class=\"data row19 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4a3b0_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "      <td id=\"T_4a3b0_row20_col0\" class=\"data row20 col0\" >Log Experiment</td>\n",
       "      <td id=\"T_4a3b0_row20_col1\" class=\"data row20 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4a3b0_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
       "      <td id=\"T_4a3b0_row21_col0\" class=\"data row21 col0\" >Experiment Name</td>\n",
       "      <td id=\"T_4a3b0_row21_col1\" class=\"data row21 col1\" >clf-default-name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4a3b0_level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
       "      <td id=\"T_4a3b0_row22_col0\" class=\"data row22 col0\" >USI</td>\n",
       "      <td id=\"T_4a3b0_row22_col1\" class=\"data row22 col1\" >7552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f3936c465b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pycaret.classification import *\n",
    "\n",
    "clf = setup(data = train, target = 'nerdiness', remove_outliers=True,fix_imbalance=True,train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_8b6f6 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_8b6f6_row0_col0, #T_8b6f6_row0_col1, #T_8b6f6_row0_col3, #T_8b6f6_row0_col4, #T_8b6f6_row0_col5, #T_8b6f6_row0_col6, #T_8b6f6_row0_col7, #T_8b6f6_row1_col0, #T_8b6f6_row1_col2, #T_8b6f6_row1_col3, #T_8b6f6_row2_col0, #T_8b6f6_row2_col1, #T_8b6f6_row2_col2, #T_8b6f6_row2_col3, #T_8b6f6_row2_col4, #T_8b6f6_row2_col5, #T_8b6f6_row2_col6, #T_8b6f6_row2_col7, #T_8b6f6_row3_col0, #T_8b6f6_row3_col1, #T_8b6f6_row3_col2, #T_8b6f6_row3_col3, #T_8b6f6_row3_col4, #T_8b6f6_row3_col5, #T_8b6f6_row3_col6, #T_8b6f6_row3_col7, #T_8b6f6_row4_col0, #T_8b6f6_row4_col1, #T_8b6f6_row4_col2, #T_8b6f6_row4_col3, #T_8b6f6_row4_col4, #T_8b6f6_row4_col5, #T_8b6f6_row4_col6, #T_8b6f6_row4_col7, #T_8b6f6_row5_col0, #T_8b6f6_row5_col1, #T_8b6f6_row5_col2, #T_8b6f6_row5_col3, #T_8b6f6_row5_col4, #T_8b6f6_row5_col5, #T_8b6f6_row5_col6, #T_8b6f6_row5_col7, #T_8b6f6_row6_col0, #T_8b6f6_row6_col1, #T_8b6f6_row6_col2, #T_8b6f6_row6_col3, #T_8b6f6_row6_col4, #T_8b6f6_row6_col5, #T_8b6f6_row6_col6, #T_8b6f6_row6_col7, #T_8b6f6_row7_col0, #T_8b6f6_row7_col1, #T_8b6f6_row7_col2, #T_8b6f6_row7_col3, #T_8b6f6_row7_col4, #T_8b6f6_row7_col5, #T_8b6f6_row7_col6, #T_8b6f6_row7_col7, #T_8b6f6_row8_col0, #T_8b6f6_row8_col1, #T_8b6f6_row8_col2, #T_8b6f6_row8_col3, #T_8b6f6_row8_col4, #T_8b6f6_row8_col5, #T_8b6f6_row8_col6, #T_8b6f6_row8_col7, #T_8b6f6_row9_col0, #T_8b6f6_row9_col1, #T_8b6f6_row9_col2, #T_8b6f6_row9_col3, #T_8b6f6_row9_col4, #T_8b6f6_row9_col5, #T_8b6f6_row9_col6, #T_8b6f6_row9_col7, #T_8b6f6_row10_col0, #T_8b6f6_row10_col1, #T_8b6f6_row10_col2, #T_8b6f6_row10_col3, #T_8b6f6_row10_col4, #T_8b6f6_row10_col5, #T_8b6f6_row10_col6, #T_8b6f6_row10_col7, #T_8b6f6_row11_col0, #T_8b6f6_row11_col1, #T_8b6f6_row11_col2, #T_8b6f6_row11_col3, #T_8b6f6_row11_col4, #T_8b6f6_row11_col5, #T_8b6f6_row11_col6, #T_8b6f6_row11_col7, #T_8b6f6_row12_col0, #T_8b6f6_row12_col1, #T_8b6f6_row12_col2, #T_8b6f6_row12_col4, #T_8b6f6_row12_col5, #T_8b6f6_row12_col6, #T_8b6f6_row12_col7, #T_8b6f6_row13_col0, #T_8b6f6_row13_col1, #T_8b6f6_row13_col2, #T_8b6f6_row13_col3, #T_8b6f6_row13_col4, #T_8b6f6_row13_col5, #T_8b6f6_row13_col6, #T_8b6f6_row13_col7, #T_8b6f6_row14_col0, #T_8b6f6_row14_col1, #T_8b6f6_row14_col2, #T_8b6f6_row14_col3, #T_8b6f6_row14_col4, #T_8b6f6_row14_col5, #T_8b6f6_row14_col6, #T_8b6f6_row14_col7 {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_8b6f6_row0_col2, #T_8b6f6_row1_col1, #T_8b6f6_row1_col4, #T_8b6f6_row1_col5, #T_8b6f6_row1_col6, #T_8b6f6_row1_col7, #T_8b6f6_row12_col3 {\n",
       "  text-align: left;\n",
       "  background-color: yellow;\n",
       "}\n",
       "#T_8b6f6_row0_col8, #T_8b6f6_row1_col8, #T_8b6f6_row2_col8, #T_8b6f6_row3_col8, #T_8b6f6_row4_col8, #T_8b6f6_row5_col8, #T_8b6f6_row6_col8, #T_8b6f6_row7_col8, #T_8b6f6_row9_col8, #T_8b6f6_row10_col8, #T_8b6f6_row11_col8, #T_8b6f6_row12_col8, #T_8b6f6_row13_col8 {\n",
       "  text-align: left;\n",
       "  background-color: lightgrey;\n",
       "}\n",
       "#T_8b6f6_row8_col8, #T_8b6f6_row14_col8 {\n",
       "  text-align: left;\n",
       "  background-color: yellow;\n",
       "  background-color: lightgrey;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_8b6f6\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_8b6f6_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_8b6f6_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_8b6f6_level0_col2\" class=\"col_heading level0 col2\" >AUC</th>\n",
       "      <th id=\"T_8b6f6_level0_col3\" class=\"col_heading level0 col3\" >Recall</th>\n",
       "      <th id=\"T_8b6f6_level0_col4\" class=\"col_heading level0 col4\" >Prec.</th>\n",
       "      <th id=\"T_8b6f6_level0_col5\" class=\"col_heading level0 col5\" >F1</th>\n",
       "      <th id=\"T_8b6f6_level0_col6\" class=\"col_heading level0 col6\" >Kappa</th>\n",
       "      <th id=\"T_8b6f6_level0_col7\" class=\"col_heading level0 col7\" >MCC</th>\n",
       "      <th id=\"T_8b6f6_level0_col8\" class=\"col_heading level0 col8\" >TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_8b6f6_level0_row0\" class=\"row_heading level0 row0\" >et</th>\n",
       "      <td id=\"T_8b6f6_row0_col0\" class=\"data row0 col0\" >Extra Trees Classifier</td>\n",
       "      <td id=\"T_8b6f6_row0_col1\" class=\"data row0 col1\" >0.7758</td>\n",
       "      <td id=\"T_8b6f6_row0_col2\" class=\"data row0 col2\" >0.8686</td>\n",
       "      <td id=\"T_8b6f6_row0_col3\" class=\"data row0 col3\" >0.8327</td>\n",
       "      <td id=\"T_8b6f6_row0_col4\" class=\"data row0 col4\" >0.7780</td>\n",
       "      <td id=\"T_8b6f6_row0_col5\" class=\"data row0 col5\" >0.8044</td>\n",
       "      <td id=\"T_8b6f6_row0_col6\" class=\"data row0 col6\" >0.5425</td>\n",
       "      <td id=\"T_8b6f6_row0_col7\" class=\"data row0 col7\" >0.5444</td>\n",
       "      <td id=\"T_8b6f6_row0_col8\" class=\"data row0 col8\" >0.0630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8b6f6_level0_row1\" class=\"row_heading level0 row1\" >rf</th>\n",
       "      <td id=\"T_8b6f6_row1_col0\" class=\"data row1 col0\" >Random Forest Classifier</td>\n",
       "      <td id=\"T_8b6f6_row1_col1\" class=\"data row1 col1\" >0.7774</td>\n",
       "      <td id=\"T_8b6f6_row1_col2\" class=\"data row1 col2\" >0.8625</td>\n",
       "      <td id=\"T_8b6f6_row1_col3\" class=\"data row1 col3\" >0.8338</td>\n",
       "      <td id=\"T_8b6f6_row1_col4\" class=\"data row1 col4\" >0.7797</td>\n",
       "      <td id=\"T_8b6f6_row1_col5\" class=\"data row1 col5\" >0.8057</td>\n",
       "      <td id=\"T_8b6f6_row1_col6\" class=\"data row1 col6\" >0.5458</td>\n",
       "      <td id=\"T_8b6f6_row1_col7\" class=\"data row1 col7\" >0.5478</td>\n",
       "      <td id=\"T_8b6f6_row1_col8\" class=\"data row1 col8\" >0.0680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8b6f6_level0_row2\" class=\"row_heading level0 row2\" >catboost</th>\n",
       "      <td id=\"T_8b6f6_row2_col0\" class=\"data row2 col0\" >CatBoost Classifier</td>\n",
       "      <td id=\"T_8b6f6_row2_col1\" class=\"data row2 col1\" >0.7536</td>\n",
       "      <td id=\"T_8b6f6_row2_col2\" class=\"data row2 col2\" >0.8312</td>\n",
       "      <td id=\"T_8b6f6_row2_col3\" class=\"data row2 col3\" >0.8187</td>\n",
       "      <td id=\"T_8b6f6_row2_col4\" class=\"data row2 col4\" >0.7565</td>\n",
       "      <td id=\"T_8b6f6_row2_col5\" class=\"data row2 col5\" >0.7863</td>\n",
       "      <td id=\"T_8b6f6_row2_col6\" class=\"data row2 col6\" >0.4965</td>\n",
       "      <td id=\"T_8b6f6_row2_col7\" class=\"data row2 col7\" >0.4988</td>\n",
       "      <td id=\"T_8b6f6_row2_col8\" class=\"data row2 col8\" >7.6240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8b6f6_level0_row3\" class=\"row_heading level0 row3\" >lightgbm</th>\n",
       "      <td id=\"T_8b6f6_row3_col0\" class=\"data row3 col0\" >Light Gradient Boosting Machine</td>\n",
       "      <td id=\"T_8b6f6_row3_col1\" class=\"data row3 col1\" >0.7486</td>\n",
       "      <td id=\"T_8b6f6_row3_col2\" class=\"data row3 col2\" >0.8211</td>\n",
       "      <td id=\"T_8b6f6_row3_col3\" class=\"data row3 col3\" >0.8127</td>\n",
       "      <td id=\"T_8b6f6_row3_col4\" class=\"data row3 col4\" >0.7530</td>\n",
       "      <td id=\"T_8b6f6_row3_col5\" class=\"data row3 col5\" >0.7817</td>\n",
       "      <td id=\"T_8b6f6_row3_col6\" class=\"data row3 col6\" >0.4865</td>\n",
       "      <td id=\"T_8b6f6_row3_col7\" class=\"data row3 col7\" >0.4885</td>\n",
       "      <td id=\"T_8b6f6_row3_col8\" class=\"data row3 col8\" >0.0750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8b6f6_level0_row4\" class=\"row_heading level0 row4\" >gbc</th>\n",
       "      <td id=\"T_8b6f6_row4_col0\" class=\"data row4 col0\" >Gradient Boosting Classifier</td>\n",
       "      <td id=\"T_8b6f6_row4_col1\" class=\"data row4 col1\" >0.7307</td>\n",
       "      <td id=\"T_8b6f6_row4_col2\" class=\"data row4 col2\" >0.8043</td>\n",
       "      <td id=\"T_8b6f6_row4_col3\" class=\"data row4 col3\" >0.7994</td>\n",
       "      <td id=\"T_8b6f6_row4_col4\" class=\"data row4 col4\" >0.7367</td>\n",
       "      <td id=\"T_8b6f6_row4_col5\" class=\"data row4 col5\" >0.7667</td>\n",
       "      <td id=\"T_8b6f6_row4_col6\" class=\"data row4 col6\" >0.4495</td>\n",
       "      <td id=\"T_8b6f6_row4_col7\" class=\"data row4 col7\" >0.4517</td>\n",
       "      <td id=\"T_8b6f6_row4_col8\" class=\"data row4 col8\" >0.3420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8b6f6_level0_row5\" class=\"row_heading level0 row5\" >lr</th>\n",
       "      <td id=\"T_8b6f6_row5_col0\" class=\"data row5 col0\" >Logistic Regression</td>\n",
       "      <td id=\"T_8b6f6_row5_col1\" class=\"data row5 col1\" >0.7270</td>\n",
       "      <td id=\"T_8b6f6_row5_col2\" class=\"data row5 col2\" >0.7959</td>\n",
       "      <td id=\"T_8b6f6_row5_col3\" class=\"data row5 col3\" >0.7946</td>\n",
       "      <td id=\"T_8b6f6_row5_col4\" class=\"data row5 col4\" >0.7343</td>\n",
       "      <td id=\"T_8b6f6_row5_col5\" class=\"data row5 col5\" >0.7632</td>\n",
       "      <td id=\"T_8b6f6_row5_col6\" class=\"data row5 col6\" >0.4422</td>\n",
       "      <td id=\"T_8b6f6_row5_col7\" class=\"data row5 col7\" >0.4443</td>\n",
       "      <td id=\"T_8b6f6_row5_col8\" class=\"data row5 col8\" >0.0810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8b6f6_level0_row6\" class=\"row_heading level0 row6\" >lda</th>\n",
       "      <td id=\"T_8b6f6_row6_col0\" class=\"data row6 col0\" >Linear Discriminant Analysis</td>\n",
       "      <td id=\"T_8b6f6_row6_col1\" class=\"data row6 col1\" >0.7265</td>\n",
       "      <td id=\"T_8b6f6_row6_col2\" class=\"data row6 col2\" >0.7957</td>\n",
       "      <td id=\"T_8b6f6_row6_col3\" class=\"data row6 col3\" >0.7981</td>\n",
       "      <td id=\"T_8b6f6_row6_col4\" class=\"data row6 col4\" >0.7322</td>\n",
       "      <td id=\"T_8b6f6_row6_col5\" class=\"data row6 col5\" >0.7637</td>\n",
       "      <td id=\"T_8b6f6_row6_col6\" class=\"data row6 col6\" >0.4406</td>\n",
       "      <td id=\"T_8b6f6_row6_col7\" class=\"data row6 col7\" >0.4431</td>\n",
       "      <td id=\"T_8b6f6_row6_col8\" class=\"data row6 col8\" >0.0310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8b6f6_level0_row7\" class=\"row_heading level0 row7\" >ada</th>\n",
       "      <td id=\"T_8b6f6_row7_col0\" class=\"data row7 col0\" >Ada Boost Classifier</td>\n",
       "      <td id=\"T_8b6f6_row7_col1\" class=\"data row7 col1\" >0.7248</td>\n",
       "      <td id=\"T_8b6f6_row7_col2\" class=\"data row7 col2\" >0.7936</td>\n",
       "      <td id=\"T_8b6f6_row7_col3\" class=\"data row7 col3\" >0.7881</td>\n",
       "      <td id=\"T_8b6f6_row7_col4\" class=\"data row7 col4\" >0.7344</td>\n",
       "      <td id=\"T_8b6f6_row7_col5\" class=\"data row7 col5\" >0.7602</td>\n",
       "      <td id=\"T_8b6f6_row7_col6\" class=\"data row7 col6\" >0.4382</td>\n",
       "      <td id=\"T_8b6f6_row7_col7\" class=\"data row7 col7\" >0.4398</td>\n",
       "      <td id=\"T_8b6f6_row7_col8\" class=\"data row7 col8\" >0.0950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8b6f6_level0_row8\" class=\"row_heading level0 row8\" >nb</th>\n",
       "      <td id=\"T_8b6f6_row8_col0\" class=\"data row8 col0\" >Naive Bayes</td>\n",
       "      <td id=\"T_8b6f6_row8_col1\" class=\"data row8 col1\" >0.6941</td>\n",
       "      <td id=\"T_8b6f6_row8_col2\" class=\"data row8 col2\" >0.7548</td>\n",
       "      <td id=\"T_8b6f6_row8_col3\" class=\"data row8 col3\" >0.7133</td>\n",
       "      <td id=\"T_8b6f6_row8_col4\" class=\"data row8 col4\" >0.7287</td>\n",
       "      <td id=\"T_8b6f6_row8_col5\" class=\"data row8 col5\" >0.7208</td>\n",
       "      <td id=\"T_8b6f6_row8_col6\" class=\"data row8 col6\" >0.3826</td>\n",
       "      <td id=\"T_8b6f6_row8_col7\" class=\"data row8 col7\" >0.3828</td>\n",
       "      <td id=\"T_8b6f6_row8_col8\" class=\"data row8 col8\" >0.0240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8b6f6_level0_row9\" class=\"row_heading level0 row9\" >knn</th>\n",
       "      <td id=\"T_8b6f6_row9_col0\" class=\"data row9 col0\" >K Neighbors Classifier</td>\n",
       "      <td id=\"T_8b6f6_row9_col1\" class=\"data row9 col1\" >0.6786</td>\n",
       "      <td id=\"T_8b6f6_row9_col2\" class=\"data row9 col2\" >0.7308</td>\n",
       "      <td id=\"T_8b6f6_row9_col3\" class=\"data row9 col3\" >0.7726</td>\n",
       "      <td id=\"T_8b6f6_row9_col4\" class=\"data row9 col4\" >0.6864</td>\n",
       "      <td id=\"T_8b6f6_row9_col5\" class=\"data row9 col5\" >0.7269</td>\n",
       "      <td id=\"T_8b6f6_row9_col6\" class=\"data row9 col6\" >0.3397</td>\n",
       "      <td id=\"T_8b6f6_row9_col7\" class=\"data row9 col7\" >0.3433</td>\n",
       "      <td id=\"T_8b6f6_row9_col8\" class=\"data row9 col8\" >0.0960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8b6f6_level0_row10\" class=\"row_heading level0 row10\" >qda</th>\n",
       "      <td id=\"T_8b6f6_row10_col0\" class=\"data row10 col0\" >Quadratic Discriminant Analysis</td>\n",
       "      <td id=\"T_8b6f6_row10_col1\" class=\"data row10 col1\" >0.6669</td>\n",
       "      <td id=\"T_8b6f6_row10_col2\" class=\"data row10 col2\" >0.7187</td>\n",
       "      <td id=\"T_8b6f6_row10_col3\" class=\"data row10 col3\" >0.7940</td>\n",
       "      <td id=\"T_8b6f6_row10_col4\" class=\"data row10 col4\" >0.6680</td>\n",
       "      <td id=\"T_8b6f6_row10_col5\" class=\"data row10 col5\" >0.7252</td>\n",
       "      <td id=\"T_8b6f6_row10_col6\" class=\"data row10 col6\" >0.3102</td>\n",
       "      <td id=\"T_8b6f6_row10_col7\" class=\"data row10 col7\" >0.3185</td>\n",
       "      <td id=\"T_8b6f6_row10_col8\" class=\"data row10 col8\" >0.0280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8b6f6_level0_row11\" class=\"row_heading level0 row11\" >dt</th>\n",
       "      <td id=\"T_8b6f6_row11_col0\" class=\"data row11 col0\" >Decision Tree Classifier</td>\n",
       "      <td id=\"T_8b6f6_row11_col1\" class=\"data row11 col1\" >0.7053</td>\n",
       "      <td id=\"T_8b6f6_row11_col2\" class=\"data row11 col2\" >0.7018</td>\n",
       "      <td id=\"T_8b6f6_row11_col3\" class=\"data row11 col3\" >0.7344</td>\n",
       "      <td id=\"T_8b6f6_row11_col4\" class=\"data row11 col4\" >0.7337</td>\n",
       "      <td id=\"T_8b6f6_row11_col5\" class=\"data row11 col5\" >0.7340</td>\n",
       "      <td id=\"T_8b6f6_row11_col6\" class=\"data row11 col6\" >0.4036</td>\n",
       "      <td id=\"T_8b6f6_row11_col7\" class=\"data row11 col7\" >0.4036</td>\n",
       "      <td id=\"T_8b6f6_row11_col8\" class=\"data row11 col8\" >0.0430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8b6f6_level0_row12\" class=\"row_heading level0 row12\" >dummy</th>\n",
       "      <td id=\"T_8b6f6_row12_col0\" class=\"data row12 col0\" >Dummy Classifier</td>\n",
       "      <td id=\"T_8b6f6_row12_col1\" class=\"data row12 col1\" >0.5537</td>\n",
       "      <td id=\"T_8b6f6_row12_col2\" class=\"data row12 col2\" >0.5000</td>\n",
       "      <td id=\"T_8b6f6_row12_col3\" class=\"data row12 col3\" >1.0000</td>\n",
       "      <td id=\"T_8b6f6_row12_col4\" class=\"data row12 col4\" >0.5537</td>\n",
       "      <td id=\"T_8b6f6_row12_col5\" class=\"data row12 col5\" >0.7127</td>\n",
       "      <td id=\"T_8b6f6_row12_col6\" class=\"data row12 col6\" >0.0000</td>\n",
       "      <td id=\"T_8b6f6_row12_col7\" class=\"data row12 col7\" >0.0000</td>\n",
       "      <td id=\"T_8b6f6_row12_col8\" class=\"data row12 col8\" >0.0270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8b6f6_level0_row13\" class=\"row_heading level0 row13\" >svm</th>\n",
       "      <td id=\"T_8b6f6_row13_col0\" class=\"data row13 col0\" >SVM - Linear Kernel</td>\n",
       "      <td id=\"T_8b6f6_row13_col1\" class=\"data row13 col1\" >0.6414</td>\n",
       "      <td id=\"T_8b6f6_row13_col2\" class=\"data row13 col2\" >0.0000</td>\n",
       "      <td id=\"T_8b6f6_row13_col3\" class=\"data row13 col3\" >0.6677</td>\n",
       "      <td id=\"T_8b6f6_row13_col4\" class=\"data row13 col4\" >0.7238</td>\n",
       "      <td id=\"T_8b6f6_row13_col5\" class=\"data row13 col5\" >0.6399</td>\n",
       "      <td id=\"T_8b6f6_row13_col6\" class=\"data row13 col6\" >0.2777</td>\n",
       "      <td id=\"T_8b6f6_row13_col7\" class=\"data row13 col7\" >0.3226</td>\n",
       "      <td id=\"T_8b6f6_row13_col8\" class=\"data row13 col8\" >0.0740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8b6f6_level0_row14\" class=\"row_heading level0 row14\" >ridge</th>\n",
       "      <td id=\"T_8b6f6_row14_col0\" class=\"data row14 col0\" >Ridge Classifier</td>\n",
       "      <td id=\"T_8b6f6_row14_col1\" class=\"data row14 col1\" >0.7267</td>\n",
       "      <td id=\"T_8b6f6_row14_col2\" class=\"data row14 col2\" >0.0000</td>\n",
       "      <td id=\"T_8b6f6_row14_col3\" class=\"data row14 col3\" >0.7994</td>\n",
       "      <td id=\"T_8b6f6_row14_col4\" class=\"data row14 col4\" >0.7319</td>\n",
       "      <td id=\"T_8b6f6_row14_col5\" class=\"data row14 col5\" >0.7641</td>\n",
       "      <td id=\"T_8b6f6_row14_col6\" class=\"data row14 col6\" >0.4408</td>\n",
       "      <td id=\"T_8b6f6_row14_col7\" class=\"data row14 col7\" >0.4434</td>\n",
       "      <td id=\"T_8b6f6_row14_col8\" class=\"data row14 col8\" >0.0240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f38a1513070>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a99ef359ebf34b8e81fc75a5c67135a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/67 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_3 = compare_models(sort = 'AUC', n_select = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_9a033_row5_col0, #T_9a033_row5_col1, #T_9a033_row5_col2, #T_9a033_row5_col3, #T_9a033_row5_col4, #T_9a033_row5_col5, #T_9a033_row5_col6 {\n",
       "  background: yellow;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_9a033\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_9a033_level0_col0\" class=\"col_heading level0 col0\" >Accuracy</th>\n",
       "      <th id=\"T_9a033_level0_col1\" class=\"col_heading level0 col1\" >AUC</th>\n",
       "      <th id=\"T_9a033_level0_col2\" class=\"col_heading level0 col2\" >Recall</th>\n",
       "      <th id=\"T_9a033_level0_col3\" class=\"col_heading level0 col3\" >Prec.</th>\n",
       "      <th id=\"T_9a033_level0_col4\" class=\"col_heading level0 col4\" >F1</th>\n",
       "      <th id=\"T_9a033_level0_col5\" class=\"col_heading level0 col5\" >Kappa</th>\n",
       "      <th id=\"T_9a033_level0_col6\" class=\"col_heading level0 col6\" >MCC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Fold</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "      <th class=\"blank col6\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_9a033_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_9a033_row0_col0\" class=\"data row0 col0\" >0.7895</td>\n",
       "      <td id=\"T_9a033_row0_col1\" class=\"data row0 col1\" >0.8697</td>\n",
       "      <td id=\"T_9a033_row0_col2\" class=\"data row0 col2\" >0.8569</td>\n",
       "      <td id=\"T_9a033_row0_col3\" class=\"data row0 col3\" >0.7832</td>\n",
       "      <td id=\"T_9a033_row0_col4\" class=\"data row0 col4\" >0.8184</td>\n",
       "      <td id=\"T_9a033_row0_col5\" class=\"data row0 col5\" >0.5692</td>\n",
       "      <td id=\"T_9a033_row0_col6\" class=\"data row0 col6\" >0.5725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9a033_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_9a033_row1_col0\" class=\"data row1 col0\" >0.7774</td>\n",
       "      <td id=\"T_9a033_row1_col1\" class=\"data row1 col1\" >0.8626</td>\n",
       "      <td id=\"T_9a033_row1_col2\" class=\"data row1 col2\" >0.8381</td>\n",
       "      <td id=\"T_9a033_row1_col3\" class=\"data row1 col3\" >0.7772</td>\n",
       "      <td id=\"T_9a033_row1_col4\" class=\"data row1 col4\" >0.8065</td>\n",
       "      <td id=\"T_9a033_row1_col5\" class=\"data row1 col5\" >0.5454</td>\n",
       "      <td id=\"T_9a033_row1_col6\" class=\"data row1 col6\" >0.5475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9a033_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_9a033_row2_col0\" class=\"data row2 col0\" >0.7762</td>\n",
       "      <td id=\"T_9a033_row2_col1\" class=\"data row2 col1\" >0.8568</td>\n",
       "      <td id=\"T_9a033_row2_col2\" class=\"data row2 col2\" >0.8396</td>\n",
       "      <td id=\"T_9a033_row2_col3\" class=\"data row2 col3\" >0.7748</td>\n",
       "      <td id=\"T_9a033_row2_col4\" class=\"data row2 col4\" >0.8059</td>\n",
       "      <td id=\"T_9a033_row2_col5\" class=\"data row2 col5\" >0.5425</td>\n",
       "      <td id=\"T_9a033_row2_col6\" class=\"data row2 col6\" >0.5450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9a033_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_9a033_row3_col0\" class=\"data row3 col0\" >0.7757</td>\n",
       "      <td id=\"T_9a033_row3_col1\" class=\"data row3 col1\" >0.8612</td>\n",
       "      <td id=\"T_9a033_row3_col2\" class=\"data row3 col2\" >0.8232</td>\n",
       "      <td id=\"T_9a033_row3_col3\" class=\"data row3 col3\" >0.7831</td>\n",
       "      <td id=\"T_9a033_row3_col4\" class=\"data row3 col4\" >0.8026</td>\n",
       "      <td id=\"T_9a033_row3_col5\" class=\"data row3 col5\" >0.5434</td>\n",
       "      <td id=\"T_9a033_row3_col6\" class=\"data row3 col6\" >0.5443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9a033_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_9a033_row4_col0\" class=\"data row4 col0\" >0.7794</td>\n",
       "      <td id=\"T_9a033_row4_col1\" class=\"data row4 col1\" >0.8703</td>\n",
       "      <td id=\"T_9a033_row4_col2\" class=\"data row4 col2\" >0.8592</td>\n",
       "      <td id=\"T_9a033_row4_col3\" class=\"data row4 col3\" >0.7694</td>\n",
       "      <td id=\"T_9a033_row4_col4\" class=\"data row4 col4\" >0.8118</td>\n",
       "      <td id=\"T_9a033_row4_col5\" class=\"data row4 col5\" >0.5473</td>\n",
       "      <td id=\"T_9a033_row4_col6\" class=\"data row4 col6\" >0.5521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9a033_level0_row5\" class=\"row_heading level0 row5\" >Mean</th>\n",
       "      <td id=\"T_9a033_row5_col0\" class=\"data row5 col0\" >0.7796</td>\n",
       "      <td id=\"T_9a033_row5_col1\" class=\"data row5 col1\" >0.8641</td>\n",
       "      <td id=\"T_9a033_row5_col2\" class=\"data row5 col2\" >0.8434</td>\n",
       "      <td id=\"T_9a033_row5_col3\" class=\"data row5 col3\" >0.7776</td>\n",
       "      <td id=\"T_9a033_row5_col4\" class=\"data row5 col4\" >0.8091</td>\n",
       "      <td id=\"T_9a033_row5_col5\" class=\"data row5 col5\" >0.5496</td>\n",
       "      <td id=\"T_9a033_row5_col6\" class=\"data row5 col6\" >0.5523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9a033_level0_row6\" class=\"row_heading level0 row6\" >Std</th>\n",
       "      <td id=\"T_9a033_row6_col0\" class=\"data row6 col0\" >0.0051</td>\n",
       "      <td id=\"T_9a033_row6_col1\" class=\"data row6 col1\" >0.0052</td>\n",
       "      <td id=\"T_9a033_row6_col2\" class=\"data row6 col2\" >0.0133</td>\n",
       "      <td id=\"T_9a033_row6_col3\" class=\"data row6 col3\" >0.0052</td>\n",
       "      <td id=\"T_9a033_row6_col4\" class=\"data row6 col4\" >0.0055</td>\n",
       "      <td id=\"T_9a033_row6_col5\" class=\"data row6 col5\" >0.0100</td>\n",
       "      <td id=\"T_9a033_row6_col6\" class=\"data row6 col6\" >0.0105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f3949392b50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81a32dec13914c29a8b64a0d09263739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "blended = blend_models(estimator_list = best_3, fold = 5, method = 'soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_f6a95\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_f6a95_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_f6a95_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_f6a95_level0_col2\" class=\"col_heading level0 col2\" >AUC</th>\n",
       "      <th id=\"T_f6a95_level0_col3\" class=\"col_heading level0 col3\" >Recall</th>\n",
       "      <th id=\"T_f6a95_level0_col4\" class=\"col_heading level0 col4\" >Prec.</th>\n",
       "      <th id=\"T_f6a95_level0_col5\" class=\"col_heading level0 col5\" >F1</th>\n",
       "      <th id=\"T_f6a95_level0_col6\" class=\"col_heading level0 col6\" >Kappa</th>\n",
       "      <th id=\"T_f6a95_level0_col7\" class=\"col_heading level0 col7\" >MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_f6a95_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_f6a95_row0_col0\" class=\"data row0 col0\" >Voting Classifier</td>\n",
       "      <td id=\"T_f6a95_row0_col1\" class=\"data row0 col1\" >0.7933</td>\n",
       "      <td id=\"T_f6a95_row0_col2\" class=\"data row0 col2\" >0.8746</td>\n",
       "      <td id=\"T_f6a95_row0_col3\" class=\"data row0 col3\" >0.8458</td>\n",
       "      <td id=\"T_f6a95_row0_col4\" class=\"data row0 col4\" >0.7941</td>\n",
       "      <td id=\"T_f6a95_row0_col5\" class=\"data row0 col5\" >0.8191</td>\n",
       "      <td id=\"T_f6a95_row0_col6\" class=\"data row0 col6\" >0.5784</td>\n",
       "      <td id=\"T_f6a95_row0_col7\" class=\"data row0 col7\" >0.5800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f38a20db280>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_holdout = predict_model(blended,raw_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = finalize_model(blended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = predict_model(final_model, data = test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5488    26\n",
       "0.7209    23\n",
       "0.7903    23\n",
       "0.7469    22\n",
       "0.7951    21\n",
       "          ..\n",
       "0.9833     1\n",
       "0.9658     1\n",
       "0.9971     1\n",
       "0.9853     1\n",
       "0.9766     1\n",
       "Name: Score, Length: 4809, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions['Score'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = test_index\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"index\" : index,\n",
    "    \"nerdiness\" : predictions['Score']\n",
    "})\n",
    "submission.to_csv('./data/model_auto_ml2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_fill_na['nerdiness']\n",
    "x_train = train_fill_na.drop('nerdiness',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test_fill_na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = KFold(n_splits = 3, shuffle = True, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "params = {'bagging_temperature': 0.375906, 'depth': 9.0, 'l2_leaf_reg': 68.8, 'learning_rate': 0.011, 'subsample': 0.76046}\n",
    "\n",
    "clf1 = CatBoostClassifier(**params, iterations=5000, eval_metric='AUC',allow_writing_files=False,od_type='Iter',random_state=777)\n",
    "clf2 = LGBMClassifier()\n",
    "clf3 = GradientBoostingClassifier()\n",
    "soft_vote  = VotingClassifier([('r1',clf1), ('r2', clf2), ('r3',clf3)], voting='soft')\n",
    "soft_vote.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = soft_vote\n",
    "pred_y = model.predict_proba(test_fill_na)\n",
    "pred_y = pred_y[:,1]\n",
    "\n",
    "index = test_index\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"index\" : index,\n",
    "    \"voted\" : pred_y\n",
    "})\n",
    "submission.to_csv('./data/model1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train = pd.read_csv('./data/train.csv').drop('index',axis=1)\n",
    "# train.drop('introelapse',axis=1,inplace=True)\n",
    "# family size 이상치\n",
    "train = train.drop(1019)\n",
    "# age 이상치\n",
    "train = train.drop(train[train['age']>100].index.to_list())\n",
    "train = train.drop(train[train['familysize']>100].index.to_list())\n",
    "# train = train.drop(train[train['introelapse']>3000].index.to_list())\n",
    "# train = train.drop(train[train['testelapse']>10000].index.to_list())\n",
    "# train = train.drop(train[train['surveyelapse']>10000].index.to_list()) # 나에 대한 추가 질문\n",
    "\n",
    "test = pd.read_csv('./data/test.csv').drop('index',axis=1)\n",
    "# test.drop('introelapse',axis=1,inplace=True)\n",
    "test.loc[test['familysize']>100,'familysize']= train['familysize'].mean()\n",
    "test.loc[test['age']>100,'age']= train['age'].mean()\n",
    "# test.loc[test['introelapse']>3000,'introelapse']= train['introelapse'].mean()\n",
    "# test.loc[test['testelapse']>10000,'testelapse']= train['testelapse'].mean()\n",
    "# test.loc[test['surveyelapse']>10000,'surveyelapse']= train['surveyelapse'].mean()\n",
    "\n",
    "test_index = pd.read_csv('./data/test.csv')['index']\n",
    "\n",
    "pd.set_option('display.max_row', 100)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "value = train['country'].value_counts().values\n",
    "rank = list(map(lambda x : 1 if x>1000 else (2 if x>100 else 3),value))\n",
    "# rank = list(map(lambda x : 1 if x>1000 else (2 if x>500 else (3 if x>200 else (4 if x>100 else 5))),value))\n",
    "\n",
    "temp_dict = {i : 0 for i in train['country'].value_counts().index.to_list()}\n",
    "\n",
    "rank_dict = dict(zip(train['country'].value_counts().index.to_list(), rank))\n",
    "rank_dict['nan'] = 0\n",
    "train['country'] = train['country'].fillna('nan')\n",
    "train['country'] = train['country'].apply(lambda x : rank_dict[x])\n",
    "train['Ex'] = (train['TIPI1']+train['TIPI6'])/2\n",
    "train['Ag'] = (train['TIPI7']+train['TIPI2'])/2\n",
    "train['Con'] = (train['TIPI3']+train['TIPI8'])/2\n",
    "train['Es'] =(train['TIPI9']+train['TIPI4'])/2\n",
    "train['Op'] =(train['TIPI5']+train['TIPI10'])/2\n",
    "\n",
    "train['mach_score'] = train[train.columns[:20]].apply(lambda x : x.mean(),axis=1)\n",
    "train['T'] = train['Q1'] + train['Q2'] - train['Q3'] - train['Q6'] - train['Q7'] - train['Q10'] + train['Q12'] + train['Q15'] - train['Q16']\n",
    "train['V'] = -train['Q4'] + train['Q5'] + train['Q8'] - train['Q11'] + train['Q13'] - train['Q14'] - train['Q17'] + train['Q18'] + train['Q20']\n",
    "train['M'] = -train['Q9'] + train['Q19']\n",
    "# train['introelapse'] = np.log1p(train['introelapse'])\n",
    "# train['testelapse'] = np.log1p(train['testelapse'])\n",
    "# train['surveyelapse'] = np.log1p(train['surveyelapse'])\n",
    "# train.drop('hand',axis=1,inplace=True)\n",
    "# train.drop(['VCL7','VCL8','VCL11'],axis=1,inplace=True)\n",
    "\n",
    "## 6,7,8,9,11,12 의미 없어 보임\n",
    "# 6,8,10 실존하지 않는 단어 -> 얘네는 의미있게 작용하지 않을까?\n",
    "# -> 7,8,11 제거\n",
    "\n",
    "# train.drop([('TIPI'+str(i)) for i in range(1,11)],axis=1,inplace=True)\n",
    "# train.drop([('Q'+str(i)) for i in range(1,20)],axis=1,inplace=True)\n",
    "\n",
    "train['nature_score'] = train[[('Q'+str(i)) for i in range(20,27)]].apply(lambda x : x.mean(),axis=1)\n",
    "# train.drop([('Q'+str(i)) for i in range(20,27)],axis=1,inplace=True)\n",
    "# train.drop(['country','hand','introelapse','testelapse','surveyelapse'],axis=1,inplace=True)\n",
    "train_fill_na = train.fillna(train.mean())\n",
    "\n",
    "\n",
    "value = test['country'].value_counts().values\n",
    "rank = list(map(lambda x : 1 if x>1000 else (2 if x>100 else 3),value))\n",
    "# rank = list(map(lambda x : 1 if x>1000 else (2 if x>500 else (3 if x>200 else (4 if x>100 else 5))),value))\n",
    "\n",
    "temp_dict = {i : 0 for i in test['country'].value_counts().index.to_list()}\n",
    "\n",
    "rank_dict = dict(zip(test['country'].value_counts().index.to_list(), rank))\n",
    "rank_dict['nan'] = 0\n",
    "test['country'] = test['country'].fillna('nan')\n",
    "test['country'] = test['country'].apply(lambda x : rank_dict[x])\n",
    "\n",
    "test['Ex'] = (test['TIPI1']+test['TIPI6'])/2\n",
    "test['Ag'] = (test['TIPI7']+test['TIPI2'])/2\n",
    "test['Con'] = (test['TIPI3']+test['TIPI8'])/2\n",
    "test['Es'] =(test['TIPI9']+test['TIPI4'])/2\n",
    "test['Op'] =(test['TIPI5']+test['TIPI10'])/2\n",
    "\n",
    "test['mach_score'] = test[test.columns[:20]].apply(lambda x : x.mean(),axis=1)\n",
    "test['T'] = test['Q1'] + test['Q2'] - test['Q3'] - test['Q6'] - test['Q7'] - test['Q10'] + test['Q12'] + test['Q15'] - test['Q16']\n",
    "test['V'] = -test['Q4'] + test['Q5'] + test['Q8'] - test['Q11'] + test['Q13'] - test['Q14'] - test['Q17'] + test['Q18'] + test['Q20']\n",
    "test['M'] = -test['Q9'] + test['Q19']\n",
    "\n",
    "# test['introelapse'] = np.log1p(test['introelapse'])\n",
    "# test['testelapse'] = np.log1p(test['testelapse'])\n",
    "# test['surveyelapse'] = np.log1p(test['surveyelapse'])\n",
    "# test.drop('hand',axis=1,inplace=True)\n",
    "# test.drop(['VCL7','VCL8','VCL11'],axis=1,inplace=True)\n",
    "# test.drop([('TIPI'+str(i)) for i in range(1,11)],axis=1,inplace=True)\n",
    "# test.drop([('Q'+str(i)) for i in range(1,20)],axis=1,inplace=True)\n",
    "\n",
    "test['nature_score'] = test[[('Q'+str(i)) for i in range(20,27)]].apply(lambda x : x.mean(),axis=1)\n",
    "# test.drop(['country','hand','introelapse','testelapse','surveyelapse'],axis=1,inplace=True)\n",
    "# test.drop([('Q'+str(i)) for i in range(20,27)],axis=1,inplace=True)\n",
    "test_fill_na = test.fillna(test.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_ls = train_fill_na.columns.to_list()\n",
    "temp_ls.remove('mach_score')\n",
    "temp_ls.remove('nature_score')\n",
    "temp_ls.remove('nerdiness')\n",
    "temp_ls.remove('country')\n",
    "temp_ls.remove('introelapse')\n",
    "temp_ls.remove('testelapse')\n",
    "temp_ls.remove('surveyelapse')\n",
    "temp_ls.remove('hand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in temp_ls:\n",
    "    train_fill_na[i] = train_fill_na[i].astype(int)\n",
    "    test_fill_na[i] = test_fill_na[i].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_fill_na['nerdiness']\n",
    "x_train = train_fill_na.drop('nerdiness',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test_fill_na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from string import ascii_lowercase\n",
    "from itertools import combinations\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "import gc\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm_rfe_777(x_data, y_data, ratio=0.975, min_feats=30):\n",
    "    feats = x_data.columns.tolist()\n",
    "    archive = pd.DataFrame(columns=['model', 'n_feats', 'feats', 'score'])\n",
    "    while True:\n",
    "        # model = LGBMClassifier(objective = 'binary', num_iterations=10**3)\n",
    "        model = RandomForestClassifier(n_estimators=1000,n_jobs=32,random_state=777)\n",
    "        x_train, x_val, y_train, y_val = train_test_split(x_data[feats], y_data, random_state=777)\n",
    "        model.fit(x_train, y_train)\n",
    "        val_pred = model.predict_proba(x_val)\n",
    "        val_pred = val_pred[:,1]\n",
    "        score = roc_auc_score(y_val, val_pred)\n",
    "        n_feats = len(feats)\n",
    "        print(n_feats, score)\n",
    "        archive = archive.append({'model': model, 'n_feats': n_feats, 'feats': feats, 'score': score}, ignore_index=True)\n",
    "        feat_imp = pd.Series(model.feature_importances_, index=feats).sort_values(ascending=False)        \n",
    "        next_n_feats = int(n_feats * ratio)\n",
    "        if next_n_feats < min_feats:\n",
    "            break\n",
    "        else:\n",
    "            feats = feat_imp.iloc[:next_n_feats].index.tolist()\n",
    "    return archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74 0.8652778898004333\n",
      "72 0.8652666555284141\n",
      "70 0.8648648142600359\n",
      "68 0.8645767560031341\n",
      "66 0.8655879845139881\n",
      "64 0.8648589090657695\n",
      "62 0.8647340358114026\n",
      "60 0.8641247925980551\n",
      "58 0.8638988108955155\n",
      "56 0.8638201709913813\n",
      "54 0.8639346741484998\n",
      "52 0.8647068143061253\n",
      "50 0.8647588088214961\n",
      "48 0.8644683020694105\n",
      "46 0.8644183239618382\n",
      "44 0.8645084861962484\n",
      "42 0.8653062635387381\n",
      "40 0.8638904572060653\n",
      "39 0.8639770187122644\n",
      "38 0.8636548255519196\n",
      "37 0.8642076093469143\n",
      "36 0.8636153615707239\n",
      "35 0.8651113921279441\n",
      "34 0.8646552518781399\n",
      "33 0.8638733177397797\n",
      "32 0.8638878646817532\n",
      "31 0.8634204901599299\n",
      "30 0.8628250737429137\n"
     ]
    }
   ],
   "source": [
    "lgbm_archive_4040 = lgbm_rfe_777(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=1000, n_jobs=64, random_state=777)\n",
    "\n",
    "x_train_1 = x_train[lgbm_archive_4040.iloc[lgbm_archive_4040[lgbm_archive_4040['score']==lgbm_archive_4040['score'].max()].index[0],2]]\n",
    "\n",
    "model.fit(x_train_1, y_train)\n",
    "\n",
    "pred_y1 = model.predict_proba(test[lgbm_archive_4040.iloc[lgbm_archive_4040[lgbm_archive_4040['score']==lgbm_archive_4040['score'].max()].index[0],2]])\n",
    "pred_y1 = pred_y1[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm_rfe_1234(x_data, y_data, ratio=0.975, min_feats=30):\n",
    "    feats = x_data.columns.tolist()\n",
    "    archive = pd.DataFrame(columns=['model', 'n_feats', 'feats', 'score'])\n",
    "    while True:\n",
    "        # model = LGBMClassifier(objective = 'binary', num_iterations=10**3)\n",
    "        model = RandomForestClassifier(n_estimators=1000, n_jobs=64, random_state=1234)\n",
    "        x_train, x_val, y_train, y_val = train_test_split(x_data[feats], y_data, random_state=1234)\n",
    "        model.fit(x_train, y_train)\n",
    "        val_pred = model.predict_proba(x_val)\n",
    "        val_pred = val_pred[:,1]\n",
    "        score = roc_auc_score(y_val, val_pred)\n",
    "        n_feats = len(feats)\n",
    "        print(n_feats, score)\n",
    "        archive = archive.append({'model': model, 'n_feats': n_feats, 'feats': feats, 'score': score}, ignore_index=True)\n",
    "        feat_imp = pd.Series(model.feature_importances_, index=feats).sort_values(ascending=False)        \n",
    "        next_n_feats = int(n_feats * ratio)\n",
    "        if next_n_feats < min_feats:\n",
    "            break\n",
    "        else:\n",
    "            feats = feat_imp.iloc[:next_n_feats].index.tolist()\n",
    "    return archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_archive_1234 = lgbm_rfe_1234(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = RandomForestClassifier(n_estimators=1000, n_jobs=64, random_state=1234)\n",
    "\n",
    "x_train_2 = x_train[lgbm_archive_1234.iloc[lgbm_archive_1234[lgbm_archive_1234['score']==lgbm_archive_1234['score'].max()].index[0],2]]\n",
    "\n",
    "model2.fit(x_train_2, y_train)\n",
    "\n",
    "pred_y2 = model2.predict_proba(test[lgbm_archive_1234.iloc[lgbm_archive_1234[lgbm_archive_1234['score']==lgbm_archive_1234['score'].max()].index[0],2]])\n",
    "pred_y2 = pred_y2[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm_rfe_99087(x_data, y_data, ratio=0.975, min_feats=30):\n",
    "    feats = x_data.columns.tolist()\n",
    "    archive = pd.DataFrame(columns=['model', 'n_feats', 'feats', 'score'])\n",
    "    while True:\n",
    "        # model = LGBMClassifier(objective = 'binary', num_iterations=10**3)\n",
    "        model = RandomForestClassifier(n_estimators=1000, n_jobs=64, random_state=99087)\n",
    "        x_train, x_val, y_train, y_val = train_test_split(x_data[feats], y_data, random_state=99087)\n",
    "        model.fit(x_train, y_train)\n",
    "        val_pred = model.predict_proba(x_val)\n",
    "        val_pred = val_pred[:,1]\n",
    "        score = roc_auc_score(y_val, val_pred)\n",
    "        n_feats = len(feats)\n",
    "        print(n_feats, score)\n",
    "        archive = archive.append({'model': model, 'n_feats': n_feats, 'feats': feats, 'score': score}, ignore_index=True)\n",
    "        feat_imp = pd.Series(model.feature_importances_, index=feats).sort_values(ascending=False)        \n",
    "        next_n_feats = int(n_feats * ratio)\n",
    "        if next_n_feats < min_feats:\n",
    "            break\n",
    "        else:\n",
    "            feats = feat_imp.iloc[:next_n_feats].index.tolist()\n",
    "    return archive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_archive_99087 = lgbm_rfe_99087(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = RandomForestClassifier(n_estimators=1000, n_jobs=64, random_state=99087)\n",
    "\n",
    "x_train_3 = x_train[lgbm_archive_99087.iloc[lgbm_archive_99087[lgbm_archive_99087['score']==lgbm_archive_99087['score'].max()].index[0],2]]\n",
    "\n",
    "model3.fit(x_train_3, y_train)\n",
    "\n",
    "pred_y3 = model3.predict_proba(test[lgbm_archive_99087.iloc[lgbm_archive_99087[lgbm_archive_99087['score']==lgbm_archive_99087['score'].max()].index[0],2]])\n",
    "pred_y3 = pred_y3[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm_rfe_42(x_data, y_data, ratio=0.975, min_feats=30):\n",
    "    feats = x_data.columns.tolist()\n",
    "    archive = pd.DataFrame(columns=['model', 'n_feats', 'feats', 'score'])\n",
    "    while True:\n",
    "        # model = LGBMClassifier(objective = 'binary', num_iterations=10**3)\n",
    "        model = RandomForestClassifier(n_estimators=1000, n_jobs=64, random_state=5277)\n",
    "        x_train, x_val, y_train, y_val = train_test_split(x_data[feats], y_data, random_state=5277)\n",
    "        model.fit(x_train, y_train)\n",
    "        val_pred = model.predict_proba(x_val)\n",
    "        val_pred = val_pred[:,1]\n",
    "        score = roc_auc_score(y_val, val_pred)\n",
    "        n_feats = len(feats)\n",
    "        print(n_feats, score)\n",
    "        archive = archive.append({'model': model, 'n_feats': n_feats, 'feats': feats, 'score': score}, ignore_index=True)\n",
    "        feat_imp = pd.Series(model.feature_importances_, index=feats).sort_values(ascending=False)        \n",
    "        next_n_feats = int(n_feats * ratio)\n",
    "        if next_n_feats < min_feats:\n",
    "            break\n",
    "        else:\n",
    "            feats = feat_imp.iloc[:next_n_feats].index.tolist()\n",
    "    return archive\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_archive_42 = lgbm_rfe_42(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = ExtraTreesClassifier(n_estimators=1000, n_jobs=-1, random_state=5277)\n",
    "\n",
    "x_train_4 = x_train[lgbm_archive_42.iloc[lgbm_archive_42[lgbm_archive_42['score']==lgbm_archive_42['score'].max()].index[0],2]]\n",
    "\n",
    "model4.fit(x_train_4, y_train)\n",
    "\n",
    "pred_y4 = model4.predict_proba(test[lgbm_archive_42.iloc[lgbm_archive_42[lgbm_archive_42['score']==lgbm_archive_42['score'].max()].index[0],2]])\n",
    "pred_y4 = pred_y4[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_all = (pred_y1 + pred_y2 + pred_y3 + pred_y4) * (1/4)\n",
    "# pred_all = pred_y4\n",
    "\n",
    "index = pd.read_csv('./data/test.csv').index.to_list()\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"index\" : index,\n",
    "    \"nerdiness\" : pred_all\n",
    "})\n",
    "submission.to_csv('./data/model2_random_forest_not_remove.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train = pd.read_csv('./data/train.csv').drop('index',axis=1)\n",
    "# train.drop('introelapse',axis=1,inplace=True)\n",
    "# family size 이상치\n",
    "train = train.drop(1019)\n",
    "# age 이상치\n",
    "train = train.drop(train[train['age']>100].index.to_list())\n",
    "train = train.drop(train[train['introelapse']>3000].index.to_list())\n",
    "train = train.drop(train[train['testelapse']>10000].index.to_list())\n",
    "train = train.drop(train[train['surveyelapse']>10000].index.to_list()) # 나에 대한 추가 질문\n",
    "\n",
    "test = pd.read_csv('./data/test.csv').drop('index',axis=1)\n",
    "# test.drop('introelapse',axis=1,inplace=True)\n",
    "test.loc[test['familysize']>100,'familysize']= train['familysize'].mean()\n",
    "test.loc[test['age']>100,'age']= train['age'].mean()\n",
    "test.loc[test['introelapse']>3000,'introelapse']= train['introelapse'].mean()\n",
    "test.loc[test['testelapse']>10000,'testelapse']= train['testelapse'].mean()\n",
    "test.loc[test['surveyelapse']>10000,'surveyelapse']= train['surveyelapse'].mean()\n",
    "\n",
    "test_index = pd.read_csv('./data/test.csv')['index']\n",
    "\n",
    "pd.set_option('display.max_row', 100)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "value = train['country'].value_counts().values\n",
    "rank = list(map(lambda x : 1 if x>1000 else (2 if x>100 else 3),value))\n",
    "# rank = list(map(lambda x : 1 if x>2000 else (2 if x>1000 else (3 if x>500 else (4 if x>200 else (5 if x>100 else 6)))),value))\n",
    "\n",
    "temp_dict = {i : 0 for i in train['country'].value_counts().index.to_list()}\n",
    "\n",
    "rank_dict = dict(zip(train['country'].value_counts().index.to_list(), rank))\n",
    "rank_dict['nan'] = 0\n",
    "train['country'] = train['country'].fillna('nan')\n",
    "train['country'] = train['country'].apply(lambda x : rank_dict[x])\n",
    "train['Ex'] = (train['TIPI1']+train['TIPI6'])/2\n",
    "train['Ag'] = (train['TIPI7']+train['TIPI2'])/2\n",
    "train['Con'] = (train['TIPI3']+train['TIPI8'])/2\n",
    "train['Es'] =(train['TIPI9']+train['TIPI4'])/2\n",
    "train['Op'] =(train['TIPI5']+train['TIPI10'])/2\n",
    "\n",
    "train['mach_score'] = train[train.columns[:20]].apply(lambda x : x.mean(),axis=1)\n",
    "train['T'] = train['Q1'] + train['Q2'] - train['Q3'] - train['Q6'] - train['Q7'] - train['Q10'] + train['Q12'] + train['Q15'] - train['Q16']\n",
    "train['V'] = -train['Q4'] + train['Q5'] + train['Q8'] - train['Q11'] + train['Q13'] - train['Q14'] - train['Q17'] + train['Q18'] + train['Q20']\n",
    "train['M'] = -train['Q9'] + train['Q19']\n",
    "train['introelapse'] = np.log1p(train['introelapse'])\n",
    "train['testelapse'] = np.log1p(train['testelapse'])\n",
    "train['surveyelapse'] = np.log1p(train['surveyelapse'])\n",
    "train.drop('hand',axis=1,inplace=True)\n",
    "# train.drop(['VCL7','VCL8','VCL11'],axis=1,inplace=True)\n",
    "# train.drop([('TIPI'+str(i)) for i in range(1,11)],axis=1,inplace=True)\n",
    "# train.drop([('Q'+str(i)) for i in range(1,20)],axis=1,inplace=True)\n",
    "\n",
    "train['nature_score'] = train[[('Q'+str(i)) for i in range(20,27)]].apply(lambda x : x.mean(),axis=1)\n",
    "# train.drop([('Q'+str(i)) for i in range(20,27)],axis=1,inplace=True)\n",
    "train_fill_na = train.fillna(train.mean())\n",
    "\n",
    "\n",
    "value = test['country'].value_counts().values\n",
    "rank = list(map(lambda x : 1 if x>1000 else (2 if x>100 else 3),value))\n",
    "# rank = list(map(lambda x : 1 if x>2000 else (2 if x>1000 else (3 if x>500 else (4 if x>200 else (5 if x>100 else 6)))),value))\n",
    "\n",
    "temp_dict = {i : 0 for i in test['country'].value_counts().index.to_list()}\n",
    "\n",
    "rank_dict = dict(zip(test['country'].value_counts().index.to_list(), rank))\n",
    "rank_dict['nan'] = 0\n",
    "test['country'] = test['country'].fillna('nan')\n",
    "test['country'] = test['country'].apply(lambda x : rank_dict[x])\n",
    "\n",
    "test['Ex'] = (test['TIPI1']+test['TIPI6'])/2\n",
    "test['Ag'] = (test['TIPI7']+test['TIPI2'])/2\n",
    "test['Con'] = (test['TIPI3']+test['TIPI8'])/2\n",
    "test['Es'] =(test['TIPI9']+test['TIPI4'])/2\n",
    "test['Op'] =(test['TIPI5']+test['TIPI10'])/2\n",
    "\n",
    "test['mach_score'] = test[test.columns[:20]].apply(lambda x : x.mean(),axis=1)\n",
    "test['T'] = test['Q1'] + test['Q2'] - test['Q3'] - test['Q6'] - test['Q7'] - test['Q10'] + test['Q12'] + test['Q15'] - test['Q16']\n",
    "test['V'] = -test['Q4'] + test['Q5'] + test['Q8'] - test['Q11'] + test['Q13'] - test['Q14'] - test['Q17'] + test['Q18'] + test['Q20']\n",
    "test['M'] = -test['Q9'] + test['Q19']\n",
    "\n",
    "test['introelapse'] = np.log1p(test['introelapse'])\n",
    "test['testelapse'] = np.log1p(test['testelapse'])\n",
    "test['surveyelapse'] = np.log1p(test['surveyelapse'])\n",
    "test.drop('hand',axis=1,inplace=True)\n",
    "# test.drop(['VCL7','VCL8','VCL11'],axis=1,inplace=True)\n",
    "# test.drop([('TIPI'+str(i)) for i in range(1,11)],axis=1,inplace=True)\n",
    "# test.drop([('Q'+str(i)) for i in range(1,20)],axis=1,inplace=True)\n",
    "\n",
    "test['nature_score'] = test[[('Q'+str(i)) for i in range(20,27)]].apply(lambda x : x.mean(),axis=1)\n",
    "# test.drop([('Q'+str(i)) for i in range(20,27)],axis=1,inplace=True)\n",
    "test_fill_na = test.fillna(test.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ss = StandardScaler()\n",
    "\n",
    "y_train = train_fill_na['nerdiness']\n",
    "train_x = train_fill_na.drop('nerdiness',axis=1)\n",
    "test_x = test_fill_na.copy()\n",
    "\n",
    "ss.fit(train_x)\n",
    "train_x = ss.transform(train_x)\n",
    "test_x = ss.transform(test_x)\n",
    "# train_fill_na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print('Used',DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "train_y = y_train.copy()\n",
    "\n",
    "# train_y = 2 - train_y.to_numpy()\n",
    "# train_x = train_x.to_numpy()\n",
    "# test_x = test_x.to_numpy()\n",
    "\n",
    "train_y_t = torch.tensor(train_y, dtype=torch.float32)\n",
    "train_x_t = torch.tensor(train_x, dtype=torch.float32)\n",
    "test_x_t = torch.tensor(test_x, dtype=torch.float32)\n",
    "\n",
    "train_x_t[:, :26] = (train_x_t[:, :26] - 3.) / 2.\n",
    "test_x_t[:, :26] = (test_x_t[:, :26] - 3.) / 2\n",
    "\n",
    "train_x_t[:, 30:40] = (train_x_t[:, 30:40] - 3.) / 2.\n",
    "test_x_t[:, 30:40] = (test_x_t[:, 30:40] - 3.) / 2\n",
    "\n",
    "test_len = len(test_x_t)\n",
    "\n",
    "N_REPEAT = 5\n",
    "N_SKFOLD = 7\n",
    "N_EPOCH = 48\n",
    "BATCH_SIZE = 1024\n",
    "LOADER_PARAM = {\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'num_workers': 4, # in colab use 2\n",
    "    'pin_memory': True\n",
    "}\n",
    "prediction = np.zeros((test_len, 1), dtype=np.float32)\n",
    "\n",
    "for repeat in range(N_REPEAT):\n",
    "\n",
    "    skf, tot = StratifiedKFold(n_splits=N_SKFOLD, random_state=repeat, shuffle=True), 0.\n",
    "    for skfold, (train_idx, valid_idx) in enumerate(skf.split(train_x, train_y)):\n",
    "        train_idx, valid_idx = list(train_idx), list(valid_idx)\n",
    "        train_loader = DataLoader(TensorDataset(train_x_t[train_idx, :], train_y_t[train_idx]),\n",
    "                                  shuffle=True, drop_last=True, **LOADER_PARAM)\n",
    "        valid_loader = DataLoader(TensorDataset(train_x_t[valid_idx, :], train_y_t[valid_idx]),\n",
    "                                  shuffle=False, drop_last=False, **LOADER_PARAM)\n",
    "        test_loader = DataLoader(TensorDataset(test_x_t, torch.zeros((test_len,), dtype=torch.float32)),\n",
    "                                 shuffle=False, drop_last=False, **LOADER_PARAM)\n",
    "        model = nn.Sequential(\n",
    "            nn.Dropout(0.05),\n",
    "            nn.Linear(77, 180, bias=False),\n",
    "            nn.LeakyReLU(0.5, inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(180, 32, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.Dropout(0.5),\n",
    "            # nn.Linear(128, 32, bias=False),\n",
    "            # nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        ).to(DEVICE)\n",
    "        # criterion = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor([1.20665], device=DEVICE))\n",
    "        criterion = torch.nn.BCELoss()\n",
    "\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=1e-5, weight_decay=4e-2)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            optimizer, T_0=N_EPOCH // 4, eta_min=1.2e-5)\n",
    "        prediction_t, loss_t = np.zeros((test_len, 1), dtype=np.float32), 1.\n",
    "\n",
    "        # for epoch in range(N_EPOCH):\n",
    "        for epoch in tqdm(range(N_EPOCH), desc='{:02d}/{:02d}'.format(skfold + 1, N_SKFOLD)):\n",
    "            model.train()\n",
    "            for idx, (xx, yy) in enumerate(train_loader):\n",
    "                optimizer.zero_grad()\n",
    "                xx, yy = xx.to(DEVICE), yy.to(DEVICE)\n",
    "                pred = model(xx).squeeze()\n",
    "                loss = criterion(pred, yy)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step(epoch + idx / len(train_loader))\n",
    "\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                running_acc, running_loss, running_count = 0, 0., 0\n",
    "                for xx, yy in valid_loader:\n",
    "                    xx, yy = xx.to(DEVICE), yy.to(DEVICE)\n",
    "                    pred = model(xx).squeeze()\n",
    "                    loss = criterion(pred, yy)\n",
    "                    running_loss += loss.item() * len(yy)\n",
    "                    running_count += len(yy)\n",
    "                    # running_acc += ((torch.sigmoid(pred) > 0.5).float() == yy).sum().item()\n",
    "                    running_acc += (((pred) >\n",
    "                                    0.5).float() == yy).sum().item()\n",
    "                # print('R{:02d} S{:02d} E{:02d} | {:6.4f}, {:5.2f}%'.format(repeat + 1, skfold + 1, epoch + 1, running_loss / running_count,running_acc / running_count * 100))\n",
    "\n",
    "                if running_loss / running_count < loss_t:\n",
    "                    loss_t = running_loss / running_count\n",
    "                    for idx, (xx, _) in enumerate(test_loader):\n",
    "                        xx = xx.to(DEVICE)\n",
    "                        pred = ((model(xx).detach().to('cpu'))).numpy()\n",
    "                        prediction_t[BATCH_SIZE * idx:min(BATCH_SIZE * (idx + 1), len(prediction)), :] \\\n",
    "                            = pred[:, :].copy()\n",
    "        prediction[:, :] += prediction_t[:, :].copy() / (N_REPEAT * N_SKFOLD)\n",
    "        tot += loss_t\n",
    "    print('R{} -> {:6.4f}'.format(repeat + 1, tot / N_SKFOLD))\n",
    "\n",
    "df = pd.read_csv('./data/sample_submission.csv')\n",
    "df.iloc[:, 1:] = prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df['nerdiness']:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./data/model3.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# final Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = pd.read_csv('./data/model1.csv', index_col = 'index')\n",
    "model2 = pd.read_csv('./data/model2.csv', index_col='index')\n",
    "\n",
    "pred_y = (model1)*(0.7) + (model2)*(0.3)\n",
    "\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "index = test['index']\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'index': index,\n",
    "    'voted': pred_y['voted']\n",
    "    })\n",
    "\n",
    "submission.to_csv('./data/combined_model1_model2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_12 = pd.read_csv('./data/combined_model1_model2.csv', index_col = 'index')\n",
    "model3 = pd.read_csv('./data/model3.csv', index_col='index')\n",
    "model3['voted'] = model3['voted']-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = (model3)*(0.8) + (combined_12)*(0.2)\n",
    "\n",
    "test = pd.read_csv('./data/test_x.csv')\n",
    "index = test['index']\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'index': index,\n",
    "    'voted': pred_y['voted']\n",
    "    })\n",
    "\n",
    "submission.to_csv('./data/submission_final_fix_lgbm.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('for_pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6d5361de2d3a86ff8022af11ead2fa25aee948dfb14ed55cb3d2da795443f4cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
